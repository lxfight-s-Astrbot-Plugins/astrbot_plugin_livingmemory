# -*- coding: utf-8 -*-
"""
recall_engine.py - å›å¿†å¼•æ“
è´Ÿè´£æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ï¼Œä½¿ç”¨å¤šç­–ç•¥æ™ºèƒ½å¬å›æœ€ç›¸å…³çš„è®°å¿†ã€‚
æ”¯æŒå¯†é›†å‘é‡æ£€ç´¢ã€ç¨€ç–æ£€ç´¢å’Œæ··åˆæ£€ç´¢ã€‚
"""

import json
import math
import time
from typing import List, Dict, Any, Optional

from astrbot.api import logger
from astrbot.api.star import Context
from ...storage.faiss_manager import FaissManager, Result
from ..retrieval import SparseRetriever, ResultFusion


class RecallEngine:
    """
    å›å¿†å¼•æ“ï¼šè´Ÿè´£æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ï¼Œä½¿ç”¨å¤šç­–ç•¥æ™ºèƒ½å¬å›æœ€ç›¸å…³çš„è®°å¿†ã€‚
    æ”¯æŒå¯†é›†å‘é‡æ£€ç´¢ã€ç¨€ç–æ£€ç´¢å’Œæ··åˆæ£€ç´¢ã€‚
    """

    def __init__(
        self,
        config: Dict[str, Any],
        faiss_manager: FaissManager,
        sparse_retriever: Optional[SparseRetriever] = None,
    ):
        """
        åˆå§‹åŒ–å›å¿†å¼•æ“ã€‚

        Args:
            config (Dict[str, Any]): æ’ä»¶é…ç½®ä¸­ 'recall_engine' éƒ¨åˆ†çš„å­—å…¸ã€‚
            faiss_manager (FaissManager): æ•°æ®åº“ç®¡ç†å™¨å®ä¾‹ã€‚
            sparse_retriever (Optional[SparseRetriever]): ç¨€ç–æ£€ç´¢å™¨å®ä¾‹ã€‚
        """
        self.config = config
        self.faiss_manager = faiss_manager
        self.sparse_retriever = sparse_retriever

        # åˆå§‹åŒ–ç»“æœèåˆå™¨
        fusion_config = config.get("fusion", {})
        fusion_strategy = fusion_config.get("strategy", "rrf")
        self.result_fusion = ResultFusion(
            strategy=fusion_strategy, config=fusion_config
        )

        logger.info("RecallEngine åˆå§‹åŒ–æˆåŠŸã€‚")

    async def recall(
        self,
        context: Context,
        query: str,
        session_id: Optional[str] = None,
        persona_id: Optional[str] = None,
        k: Optional[int] = None,
    ) -> List[Result]:
        """
        æ‰§è¡Œå›å¿†æµç¨‹ï¼Œæ£€ç´¢å¹¶å¯èƒ½é‡æ’è®°å¿†ã€‚

        Args:
            query (str): ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬ã€‚
            session_id (Optional[str], optional): å½“å‰ä¼šè¯ ID. Defaults to None.
            persona_id (Optional[str], optional): å½“å‰äººæ ¼ ID. Defaults to None.
            k (Optional[int], optional): å¸Œæœ›è¿”å›çš„è®°å¿†æ•°é‡ï¼Œå¦‚æœä¸º None åˆ™ä»é…ç½®ä¸­è¯»å–.

        Returns:
            List[Result]: æœ€ç»ˆè¿”å›ç»™ä¸Šå±‚åº”ç”¨çš„è®°å¿†åˆ—è¡¨ã€‚
        """
        top_k = k if k is not None else self.config.get("top_k", 5)
        retrieval_mode = self.config.get(
            "retrieval_mode", "hybrid"
        )  # hybrid, dense, sparse

        logger.info(
            f"å¼€å§‹è®°å¿†å¬å› - æŸ¥è¯¢: '{query}', è¿”å›æ•°é‡: {top_k}, æ£€ç´¢æ¨¡å¼: {retrieval_mode}"
        )
        logger.debug(f"è¿‡æ»¤æ¡ä»¶ - ä¼šè¯ID: {session_id}, äººæ ¼ID: {persona_id}")

        # åˆ†ææŸ¥è¯¢ç‰¹å¾ï¼ˆç”¨äºè‡ªé€‚åº”ç­–ç•¥ï¼‰
        query_info = self.result_fusion.analyze_query(query)
        logger.debug(f"Query analysis: {query_info}")

        # æ ¹æ®æ£€ç´¢æ¨¡å¼æ‰§è¡Œæœç´¢
        if retrieval_mode == "hybrid" and self.sparse_retriever:
            # æ··åˆæ£€ç´¢
            logger.debug("ä½¿ç”¨æ··åˆæ£€ç´¢æ¨¡å¼...")
            return await self._hybrid_search(
                context, query, session_id, persona_id, top_k, query_info
            )
        elif retrieval_mode == "sparse" and self.sparse_retriever:
            # çº¯ç¨€ç–æ£€ç´¢
            logger.debug("ä½¿ç”¨ç¨€ç–æ£€ç´¢æ¨¡å¼...")
            return await self._sparse_search(query, session_id, persona_id, top_k)
        else:
            # çº¯å¯†é›†æ£€ç´¢ï¼ˆé»˜è®¤ï¼‰
            logger.debug("ä½¿ç”¨å¯†é›†æ£€ç´¢æ¨¡å¼...")
            return await self._dense_search(
                context, query, session_id, persona_id, top_k
            )

    async def _hybrid_search(
        self,
        context: Context,
        query: str,
        session_id: Optional[str],
        persona_id: Optional[str],
        k: int,
        query_info: Dict[str, Any],
    ) -> List[Result]:
        """æ‰§è¡Œæ··åˆæ£€ç´¢"""
        # å¹¶è¡Œæ‰§è¡Œå¯†é›†å’Œç¨€ç–æ£€ç´¢
        import asyncio

        # å¯†é›†æ£€ç´¢
        dense_task = self.faiss_manager.search_memory(
            query=query, k=k * 2, session_id=session_id, persona_id=persona_id
        )

        # ç¨€ç–æ£€ç´¢
        sparse_task = self.sparse_retriever.search(
            query=query, limit=k * 2, session_id=session_id, persona_id=persona_id
        )

        # ç­‰å¾…ä¸¤ä¸ªæ£€ç´¢å®Œæˆ
        dense_results, sparse_results = await asyncio.gather(
            dense_task, sparse_task, return_exceptions=True
        )

        # å¤„ç†å¼‚å¸¸
        if isinstance(dense_results, Exception):
            logger.error(f"å¯†é›†æ£€ç´¢å¤±è´¥: {dense_results}")
            dense_results = []
        if isinstance(sparse_results, Exception):
            logger.error(f"ç¨€ç–æ£€ç´¢å¤±è´¥: {sparse_results}")
            sparse_results = []

        logger.debug(
            f"Dense results: {len(dense_results)}, Sparse results: {len(sparse_results)}"
        )

        # è®°å½•èåˆå‰çš„è¯¦ç»†ä¿¡æ¯
        logger.debug(f"[{session_id}] èåˆå‰è¯¦ç»†ä¿¡æ¯:")
        logger.debug(f"[{session_id}] - å¯†é›†æ£€ç´¢ç»“æœæ•°é‡: {len(dense_results)}")
        logger.debug(f"[{session_id}] - ç¨€ç–æ£€ç´¢ç»“æœæ•°é‡: {len(sparse_results)}")

        # è®°å½•å¯†é›†æ£€ç´¢ç»“æœè¯¦æƒ…
        for i, result in enumerate(dense_results[:3]):  # åªè®°å½•å‰3ä¸ªç»“æœ
            logger.debug(
                f"[{session_id}] å¯†é›†ç»“æœ {i + 1}: ç›¸ä¼¼åº¦={result.similarity:.3f}, å†…å®¹é¢„è§ˆ={result.data.get('text', '')[:50]}..."
            )

        # è®°å½•ç¨€ç–æ£€ç´¢ç»“æœè¯¦æƒ…
        for i, result in enumerate(sparse_results[:3]):  # åªè®°å½•å‰3ä¸ªç»“æœ
            logger.debug(
                f"[{session_id}] ç¨€ç–ç»“æœ {i + 1}: åˆ†æ•°={result.score:.3f}, å†…å®¹é¢„è§ˆ={result.content[:50]}..."
            )

        # èåˆç»“æœ
        logger.debug(f"[{session_id}] å¼€å§‹ç»“æœèåˆ...")
        fused_results = self.result_fusion.fuse(
            dense_results=dense_results,
            sparse_results=sparse_results,
            k=k,
            query_info=query_info,
        )
        logger.debug(f"[{session_id}] èåˆå®Œæˆï¼Œè·å¾— {len(fused_results)} ä¸ªèåˆç»“æœ")

        # è½¬æ¢å› Result æ ¼å¼
        final_results = []
        for result in fused_results:
            final_results.append(
                Result(
                    data={
                        "id": result.doc_id,
                        "text": result.content,
                        "metadata": result.metadata,
                    },
                    similarity=result.final_score,
                )
            )

        # è®°å½•èåˆåçš„ç»“æœè¯¦æƒ…
        logger.debug(f"[{session_id}] èåˆåç»“æœè¯¦æƒ…:")
        for i, result in enumerate(final_results[:3]):  # åªè®°å½•å‰3ä¸ªç»“æœ
            logger.debug(
                f"[{session_id}] èåˆç»“æœ {i + 1}: æœ€ç»ˆåˆ†={result.similarity:.3f}, å†…å®¹é¢„è§ˆ={result.data.get('text', '')[:50]}..."
            )

        # åº”ç”¨ä¼ ç»Ÿçš„åŠ æƒé‡æ’ï¼ˆå¦‚æœéœ€è¦ï¼‰
        strategy = self.config.get("recall_strategy", "weighted")
        if strategy == "weighted":
            logger.debug(f"[{session_id}] å¯¹æ··åˆæ£€ç´¢ç»“æœåº”ç”¨åŠ æƒé‡æ’...")
            final_results = self._rerank_by_weighted_score(context, final_results)

            # è®°å½•é‡æ’åçš„ç»“æœ
            logger.debug(f"[{session_id}] é‡æ’åæœ€ç»ˆç»“æœ:")
            for i, result in enumerate(final_results[:3]):  # åªè®°å½•å‰3ä¸ªç»“æœ
                logger.debug(
                    f"[{session_id}] æœ€ç»ˆç»“æœ {i + 1}: åŠ æƒåˆ†={result.similarity:.3f}, å†…å®¹é¢„è§ˆ={result.data.get('text', '')[:50]}..."
                )

        logger.info(
            f"[{session_id}] ğŸ¯ æ··åˆæ£€ç´¢å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªè®°å¿†ç»“æœ"
        )
        return final_results

    async def _dense_search(
        self,
        context: Context,
        query: str,
        session_id: Optional[str],
        persona_id: Optional[str],
        k: int,
    ) -> List[Result]:
        """æ‰§è¡Œå¯†é›†æ£€ç´¢"""
        logger.debug(
            f"[{session_id}] å¼€å§‹å¯†é›†æ£€ç´¢ï¼ŒæŸ¥è¯¢: '{query[:50]}...', è¿”å›æ•°é‡: {k}"
        )
        results = await self.faiss_manager.search_memory(
            query=query, k=k, session_id=session_id, persona_id=persona_id
        )

        if not results:
            logger.info(f"[{session_id}] å¯†é›†æ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³è®°å¿†")
            return []

        logger.debug(f"[{session_id}] å¯†é›†æ£€ç´¢æ‰¾åˆ° {len(results)} ä¸ªå€™é€‰è®°å¿†")

        # è®°å½•å€™é€‰è®°å¿†è¯¦æƒ…
        for i, result in enumerate(results[:3]):  # åªè®°å½•å‰3ä¸ªç»“æœ
            logger.debug(
                f"[{session_id}] å¯†é›†å€™é€‰ {i + 1}: ç›¸ä¼¼åº¦={result.similarity:.3f}, å†…å®¹é¢„è§ˆ={result.data.get('text', '')[:50]}..."
            )

        # åº”ç”¨é‡æ’
        strategy = self.config.get("recall_strategy", "weighted")
        if strategy == "weighted":
            logger.debug(f"[{session_id}] ä½¿ç”¨ 'weighted' ç­–ç•¥è¿›è¡Œé‡æ’...")
            reranked_results = self._rerank_by_weighted_score(context, results)

            # è®°å½•é‡æ’åçš„ç»“æœ
            logger.debug(f"[{session_id}] å¯†é›†æ£€ç´¢é‡æ’åç»“æœ:")
            for i, result in enumerate(reranked_results[:3]):  # åªè®°å½•å‰3ä¸ªç»“æœ
                logger.debug(
                    f"[{session_id}] é‡æ’ç»“æœ {i + 1}: åŠ æƒåˆ†={result.similarity:.3f}, å†…å®¹é¢„è§ˆ={result.data.get('text', '')[:50]}..."
                )

            logger.info(
                f"[{session_id}] ğŸ¯ å¯†é›†æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(reranked_results)} ä¸ªè®°å¿†ç»“æœ"
            )
            return reranked_results
        else:
            logger.debug(
                f"[{session_id}] ä½¿ç”¨ 'similarity' ç­–ç•¥ï¼Œç›´æ¥è¿”å› {len(results)} ä¸ªç»“æœ"
            )
            logger.info(
                f"[{session_id}] ğŸ¯ å¯†é›†æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªè®°å¿†ç»“æœ"
            )
            return results

    async def _sparse_search(
        self, query: str, session_id: Optional[str], persona_id: Optional[str], k: int
    ) -> List[Result]:
        """æ‰§è¡Œç¨€ç–æ£€ç´¢"""
        logger.debug(
            f"[{session_id}] å¼€å§‹ç¨€ç–æ£€ç´¢ï¼ŒæŸ¥è¯¢: '{query[:50]}...', è¿”å›æ•°é‡: {k}"
        )
        sparse_results = await self.sparse_retriever.search(
            query=query, limit=k, session_id=session_id, persona_id=persona_id
        )

        if not sparse_results:
            logger.info(f"[{session_id}] ç¨€ç–æ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³è®°å¿†")
            return []

        logger.debug(f"[{session_id}] ç¨€ç–æ£€ç´¢æ‰¾åˆ° {len(sparse_results)} ä¸ªå€™é€‰è®°å¿†")

        # è®°å½•ç¨€ç–æ£€ç´¢å€™é€‰è®°å¿†è¯¦æƒ…
        for i, result in enumerate(sparse_results[:3]):  # åªè®°å½•å‰3ä¸ªç»“æœ
            logger.debug(
                f"[{session_id}] ç¨€ç–å€™é€‰ {i + 1}: åˆ†æ•°={result.score:.3f}, å†…å®¹é¢„è§ˆ={result.content[:50]}..."
            )

        # è½¬æ¢ä¸º Result æ ¼å¼
        results = []
        for result in sparse_results:
            results.append(
                Result(
                    data={
                        "id": result.doc_id,
                        "text": result.content,
                        "metadata": result.metadata,
                    },
                    similarity=result.score,
                )
            )

        logger.info(f"[{session_id}] ğŸ¯ ç¨€ç–æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªè®°å¿†ç»“æœ")
        return results

    def _rerank_by_weighted_score(
        self, context: Context, results: List[Result]
    ) -> List[Result]:
        """
        æ ¹æ®ç›¸ä¼¼åº¦ã€é‡è¦æ€§å’Œæ–°è¿‘åº¦å¯¹ç»“æœè¿›è¡ŒåŠ æƒé‡æ’ã€‚
        """
        sim_w = self.config.get("similarity_weight", 0.6)
        imp_w = self.config.get("importance_weight", 0.2)
        rec_w = self.config.get("recency_weight", 0.2)

        reranked_results = []
        current_time = time.time()

        for res in results:
            # å®‰å…¨è§£æå…ƒæ•°æ®
            metadata = res.data.get("metadata", {})
            if isinstance(metadata, str):
                try:
                    metadata = json.loads(metadata)
                except json.JSONDecodeError as e:
                    logger.warning(f"è§£æè®°å¿†å…ƒæ•°æ®å¤±è´¥: {e}")
                    metadata = {}

            # å½’ä¸€åŒ–å„é¡¹å¾—åˆ† (0-1)
            similarity_score = res.similarity
            importance_score = metadata.get("importance", 0.0)

            # è®¡ç®—æ–°è¿‘åº¦å¾—åˆ†
            last_access = metadata.get("last_access_time", current_time)
            # å¢åŠ å¥å£®æ€§æ£€æŸ¥ï¼Œä»¥é˜² last_access æ˜¯å­—ç¬¦ä¸²
            if isinstance(last_access, str):
                try:
                    last_access = float(last_access)
                except (ValueError, TypeError):
                    last_access = current_time

            hours_since_access = (current_time - last_access) / 3600
            # ä½¿ç”¨æŒ‡æ•°è¡°å‡ï¼ŒåŠè¡°æœŸçº¦ä¸º24å°æ—¶
            recency_score = math.exp(-0.028 * hours_since_access)

            # è®¡ç®—æœ€ç»ˆåŠ æƒåˆ†
            final_score = (
                similarity_score * sim_w
                + importance_score * imp_w
                + recency_score * rec_w
            )

            # ç›´æ¥ä¿®æ”¹ç°æœ‰ Result å¯¹è±¡çš„ similarity åˆ†æ•°
            res.similarity = final_score
            reranked_results.append(res)

        # æŒ‰æœ€ç»ˆå¾—åˆ†é™åºæ’åº
        reranked_results.sort(key=lambda x: x.similarity, reverse=True)

        return reranked_results
