# -*- coding: utf-8 -*-
"""
ç´¢å¼•ä¸€è‡´æ€§éªŒè¯å™¨ - æ£€æµ‹å¹¶ä¿®å¤ç´¢å¼•ä¸æ•°æ®åº“çš„ä¸ä¸€è‡´é—®é¢˜
"""

import aiosqlite
import json
from typing import Dict, Any, Tuple
from dataclasses import dataclass
from astrbot.api import logger


@dataclass
class IndexStatus:
    """ç´¢å¼•çŠ¶æ€ä¿¡æ¯"""

    is_consistent: bool  # æ˜¯å¦ä¸€è‡´
    documents_count: int  # documentsè¡¨ä¸­çš„æ–‡æ¡£æ•°
    bm25_count: int  # BM25ç´¢å¼•ä¸­çš„æ–‡æ¡£æ•°
    vector_count: int  # å‘é‡ç´¢å¼•ä¸­çš„æ–‡æ¡£æ•°
    missing_in_bm25: int  # documentsä¸­æœ‰ä½†BM25ä¸­ç¼ºå¤±çš„æ•°é‡
    missing_in_vector: int  # documentsä¸­æœ‰ä½†å‘é‡ç´¢å¼•ä¸­ç¼ºå¤±çš„æ•°é‡
    needs_rebuild: bool  # æ˜¯å¦éœ€è¦é‡å»º
    reason: str  # ä¸ä¸€è‡´çš„åŸå› æè¿°


class IndexValidator:
    """
    ç´¢å¼•ä¸€è‡´æ€§éªŒè¯å™¨

    æ£€æµ‹documentsè¡¨ä¸BM25ç´¢å¼•ã€å‘é‡ç´¢å¼•ä¹‹é—´çš„ä¸€è‡´æ€§
    """

    def __init__(self, db_path: str, faiss_db: Any):
        """
        åˆå§‹åŒ–éªŒè¯å™¨

        Args:
            db_path: SQLiteæ•°æ®åº“è·¯å¾„
            faiss_db: FaissVecDBå®ä¾‹
        """
        self.db_path = db_path
        self.faiss_db = faiss_db

    async def check_consistency(self) -> IndexStatus:
        """
        æ£€æŸ¥ç´¢å¼•ä¸€è‡´æ€§

        Returns:
            IndexStatus: ç´¢å¼•çŠ¶æ€ä¿¡æ¯
        """
        try:
            async with aiosqlite.connect(self.db_path) as db:
                # 1. è·å–documentsè¡¨ä¸­çš„æ–‡æ¡£æ•°å’ŒIDé›†åˆ
                cursor = await db.execute("SELECT COUNT(*) FROM documents")
                documents_count = (await cursor.fetchone())[0]

                cursor = await db.execute("SELECT id FROM documents")
                doc_ids = {row[0] for row in await cursor.fetchall()}

                # 2. æ£€æŸ¥BM25ç´¢å¼•ï¼ˆmemories_ftsè¡¨ï¼‰
                cursor = await db.execute("""
                    SELECT name FROM sqlite_master
                    WHERE type='table' AND name='memories_fts'
                """)
                has_fts_table = await cursor.fetchone()

                if has_fts_table:
                    cursor = await db.execute(
                        "SELECT COUNT(DISTINCT doc_id) FROM memories_fts"
                    )
                    bm25_count = (await cursor.fetchone())[0]

                    cursor = await db.execute(
                        "SELECT DISTINCT doc_id FROM memories_fts"
                    )
                    bm25_ids = {row[0] for row in await cursor.fetchall()}
                else:
                    bm25_count = 0
                    bm25_ids = set()

                # 3. æ£€æŸ¥å‘é‡ç´¢å¼•
                vector_count = 0
                vector_ids = set()

                try:
                    # é€šè¿‡document_storageè·å–å‘é‡ç´¢å¼•ä¸­çš„æ–‡æ¡£
                    if hasattr(self.faiss_db, "document_storage"):
                        doc_storage = self.faiss_db.document_storage
                        # è·å–æ‰€æœ‰æ–‡æ¡£ID
                        cursor = await db.execute("""
                            SELECT COUNT(DISTINCT id) FROM documents
                        """)
                        # ç®€åŒ–æ£€æŸ¥ï¼šå‡è®¾å‘é‡ç´¢å¼•ä¸document_storageåŒæ­¥
                        # å®é™…æ£€æŸ¥éœ€è¦éå†æ‰€æœ‰æ–‡æ¡£
                        for doc_id in doc_ids:
                            try:
                                doc = await doc_storage.get_document(doc_id)
                                if doc:
                                    vector_count += 1
                                    vector_ids.add(doc_id)
                            except Exception:
                                pass
                except Exception as e:
                    logger.warning(f"æ£€æŸ¥å‘é‡ç´¢å¼•å¤±è´¥: {e}")

                # 4. è®¡ç®—å·®å¼‚
                missing_in_bm25 = len(doc_ids - bm25_ids)
                missing_in_vector = len(doc_ids - vector_ids)

                # 5. åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å»º
                needs_rebuild = False
                reason = ""

                if documents_count == 0:
                    reason = "æ•°æ®åº“ä¸ºç©º"
                    is_consistent = True
                elif missing_in_bm25 > 0 or missing_in_vector > 0:
                    needs_rebuild = True
                    is_consistent = False
                    reasons = []
                    if missing_in_bm25 > 0:
                        reasons.append(f"BM25ç´¢å¼•ç¼ºå¤±{missing_in_bm25}æ¡æ–‡æ¡£")
                    if missing_in_vector > 0:
                        reasons.append(f"å‘é‡ç´¢å¼•ç¼ºå¤±{missing_in_vector}æ¡æ–‡æ¡£")
                    reason = "ï¼›".join(reasons)
                elif bm25_count > documents_count or vector_count > documents_count:
                    needs_rebuild = True
                    is_consistent = False
                    reason = "ç´¢å¼•ä¸­å­˜åœ¨å†—ä½™æ•°æ®"
                else:
                    is_consistent = True
                    reason = "ç´¢å¼•çŠ¶æ€æ­£å¸¸"

                return IndexStatus(
                    is_consistent=is_consistent,
                    documents_count=documents_count,
                    bm25_count=bm25_count,
                    vector_count=vector_count,
                    missing_in_bm25=missing_in_bm25,
                    missing_in_vector=missing_in_vector,
                    needs_rebuild=needs_rebuild,
                    reason=reason,
                )

        except Exception as e:
            logger.error(f"æ£€æŸ¥ç´¢å¼•ä¸€è‡´æ€§å¤±è´¥: {e}", exc_info=True)
            return IndexStatus(
                is_consistent=False,
                documents_count=0,
                bm25_count=0,
                vector_count=0,
                missing_in_bm25=0,
                missing_in_vector=0,
                needs_rebuild=True,
                reason=f"æ£€æŸ¥å¤±è´¥: {str(e)}",
            )

    async def get_migration_status(self) -> Tuple[bool, int]:
        """
        è·å–v1è¿ç§»çŠ¶æ€

        Returns:
            Tuple[bool, int]: (æ˜¯å¦éœ€è¦é‡å»º, å¾…å¤„ç†æ–‡æ¡£æ•°)
        """
        try:
            async with aiosqlite.connect(self.db_path) as db:
                # æ£€æŸ¥migration_statusè¡¨
                cursor = await db.execute("""
                    SELECT name FROM sqlite_master
                    WHERE type='table' AND name='migration_status'
                """)
                has_table = await cursor.fetchone()

                if not has_table:
                    return False, 0

                # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º
                cursor = await db.execute("""
                    SELECT value FROM migration_status
                    WHERE key='needs_index_rebuild'
                """)
                row = await cursor.fetchone()

                if not row or row[0] != "true":
                    return False, 0

                # è·å–å¾…å¤„ç†æ–‡æ¡£æ•°
                cursor = await db.execute("""
                    SELECT value FROM migration_status
                    WHERE key='pending_documents_count'
                """)
                count_row = await cursor.fetchone()
                pending_count = int(count_row[0]) if count_row else 0

                return True, pending_count

        except Exception as e:
            logger.error(f"è·å–è¿ç§»çŠ¶æ€å¤±è´¥: {e}", exc_info=True)
            return False, 0

    async def rebuild_indexes(
        self, memory_engine: Any, progress_callback=None
    ) -> Dict[str, Any]:
        """
        é‡å»ºæ‰€æœ‰ç´¢å¼•

        Args:
            memory_engine: MemoryEngineå®ä¾‹
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (current, total, message)

        Returns:
            Dict: é‡å»ºç»“æœ
        """
        try:
            logger.info("ğŸ”§ å¼€å§‹é‡å»ºç´¢å¼•...")

            async with aiosqlite.connect(self.db_path) as db:
                # 1. å…ˆè¯»å–æ‰€æœ‰æ–‡æ¡£åˆ°å†…å­˜ï¼ˆåœ¨æ¸…ç©ºå‰ï¼‰
                logger.info("ğŸ“¥ è¯»å–documentsè¡¨æ•°æ®...")
                cursor = await db.execute(
                    "SELECT id, text, metadata FROM documents ORDER BY id"
                )
                documents = await cursor.fetchall()

                if not documents:
                    return {
                        "success": True,
                        "message": "æ²¡æœ‰éœ€è¦é‡å»ºçš„æ–‡æ¡£",
                        "processed": 0,
                        "errors": 0,
                    }

                total = len(documents)
                logger.info(f"ğŸ“Š æ‰¾åˆ° {total} æ¡æ–‡æ¡£éœ€è¦é‡å»ºç´¢å¼•")

                # 2. æ¸…ç©ºæ‰€æœ‰å­˜å‚¨ï¼ˆdocumentsè¡¨ã€BM25ç´¢å¼•ã€å‘é‡ç´¢å¼•ï¼‰
                logger.info("ğŸ—‘ï¸ æ¸…ç©ºdocumentsè¡¨ã€BM25ç´¢å¼•å’Œå‘é‡ç´¢å¼•...")

                # æ¸…ç©ºBM25ç´¢å¼•
                try:
                    await db.execute("DELETE FROM memories_fts")
                except Exception as e:
                    logger.warning(f"æ¸…ç©ºBM25ç´¢å¼•å¤±è´¥: {e}")

                # æ¸…ç©ºdocumentsè¡¨
                await db.execute("DELETE FROM documents")
                await db.commit()

                # æ¸…ç©ºå‘é‡ç´¢å¼•
                try:
                    all_docs = (
                        await memory_engine.faiss_db.document_storage.get_documents(
                            metadata_filters={}
                        )
                    )
                    for doc in all_docs:
                        try:
                            await memory_engine.faiss_db.delete(doc["doc_id"])
                        except Exception as e:
                            logger.warning(
                                f"åˆ é™¤Faissæ–‡æ¡£å¤±è´¥ (doc_id={doc.get('doc_id')}): {e}"
                            )
                except Exception as e:
                    logger.warning(f"æ¸…ç©ºFaissç´¢å¼•æ—¶å‡ºé”™: {e}")

                logger.info("âœ… æ‰€æœ‰å­˜å‚¨å·²æ¸…ç©º")

                # 3. é‡å»ºæ‰€æœ‰å­˜å‚¨ï¼ˆdocumentsè¡¨ + ç´¢å¼•ï¼‰
                success_count = 0
                error_count = 0

                for i, (doc_id, text, metadata_json) in enumerate(documents, 1):
                    try:
                        # è§£æmetadata
                        metadata = json.loads(metadata_json) if metadata_json else {}

                        # ç¡®ä¿metadataåŒ…å«å¿…è¦å­—æ®µ
                        if "importance" not in metadata:
                            metadata["importance"] = 0.5
                        if "create_time" not in metadata:
                            import time

                            metadata["create_time"] = time.time()
                        if "last_access_time" not in metadata:
                            metadata["last_access_time"] = metadata.get(
                                "create_time", time.time()
                            )
                        if "session_id" not in metadata:
                            metadata["session_id"] = None
                        if "persona_id" not in metadata:
                            metadata["persona_id"] = None

                        # å…³é”®ä¿®å¤ï¼šä½¿ç”¨add_memoryé‡å»ºæ‰€æœ‰å­˜å‚¨
                        # è™½ç„¶ä¼šç”Ÿæˆæ–°çš„doc_idï¼Œä½†è¿™æ˜¯å¯ä»¥æ¥å—çš„
                        # å› ä¸ºæˆ‘ä»¬å·²ç»æ¸…ç©ºäº†æ‰€æœ‰æ•°æ®
                        await memory_engine.add_memory(
                            content=text,
                            session_id=metadata.get("session_id"),
                            persona_id=metadata.get("persona_id"),
                            importance=metadata.get("importance", 0.5),
                            metadata=metadata,
                        )

                        success_count += 1

                        # è¿›åº¦å›è°ƒ
                        if progress_callback and i % 10 == 0:
                            await progress_callback(i, total, f"å·²å¤„ç† {i}/{total} æ¡")

                        # æ¯å¤„ç†10æ¡è®°å½•æäº¤ä¸€æ¬¡
                        if i % 10 == 0:
                            logger.info(
                                f"â³ é‡å»ºè¿›åº¦: {i}/{total} ({i * 100 // total}%)"
                            )

                    except Exception as e:
                        error_count += 1
                        logger.error(f"é‡å»ºç´¢å¼•å¤±è´¥ doc_id={doc_id}: {e}")

                # 4. æ›´æ–°è¿ç§»çŠ¶æ€ï¼ˆå¦‚æœè¡¨å­˜åœ¨ï¼‰
                from datetime import datetime

                try:
                    await db.execute(
                        """
                        UPDATE migration_status
                        SET value='false', updated_at=?
                        WHERE key='needs_index_rebuild'
                    """,
                        (datetime.utcnow().isoformat(),),
                    )
                    await db.commit()
                except Exception as e:
                    # migration_statusè¡¨å¯èƒ½ä¸å­˜åœ¨ï¼Œè¿™æ˜¯æ­£å¸¸çš„
                    logger.debug(f"æ›´æ–°è¿ç§»çŠ¶æ€å¤±è´¥ï¼ˆå¯èƒ½è¡¨ä¸å­˜åœ¨ï¼‰: {e}")

                logger.info(
                    f"âœ… ç´¢å¼•é‡å»ºå®Œæˆ: æˆåŠŸ{success_count}æ¡, å¤±è´¥{error_count}æ¡"
                )

                return {
                    "success": True,
                    "message": "ç´¢å¼•é‡å»ºå®Œæˆ",
                    "processed": success_count,
                    "errors": error_count,
                    "total": total,
                }

        except Exception as e:
            logger.error(f"é‡å»ºç´¢å¼•å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "message": f"é‡å»ºç´¢å¼•å¤±è´¥: {str(e)}",
                "error": str(e),
            }
