# -*- coding: utf-8 -*-
"""
ç¨€ç–æ£€ç´¢å™¨ - åŸºäº SQLite FTS5 å’Œ BM25 çš„å…¨æ–‡æ£€ç´¢
"""

import json
import sqlite3
import math
import re
from typing import List, Dict, Any, Optional, Tuple, Set
from dataclasses import dataclass
import asyncio
import aiosqlite

from astrbot.api import logger
from ..utils.stopwords_manager import StopwordsManager

try:
    import jieba

    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    logger.warning("jieba not available, Chinese tokenization disabled")


@dataclass
class SparseResult:
    """ç¨€ç–æ£€ç´¢ç»“æœ"""

    doc_id: int
    score: float
    content: str
    metadata: Dict[str, Any]


class FTSManager:
    """FTS5 ç´¢å¼•ç®¡ç†å™¨"""

    def __init__(
        self, db_path: str, stopwords_manager: Optional[StopwordsManager] = None
    ):
        self.db_path = db_path
        self.fts_table_name = "documents_fts"
        self.stopwords_manager = stopwords_manager
        self.use_stopwords = stopwords_manager is not None

    async def initialize(self):
        """åˆå§‹åŒ– FTS5 ç´¢å¼•"""
        async with aiosqlite.connect(self.db_path) as db:
            # å¯ç”¨ FTS5 æ‰©å±•
            await db.execute("PRAGMA foreign_keys = ON")

            # åˆ›å»º FTS5 è™šæ‹Ÿè¡¨
            await db.execute(f"""
                CREATE VIRTUAL TABLE IF NOT EXISTS {self.fts_table_name} 
                USING fts5(content, doc_id, tokenize='unicode61')
            """)

            # åˆ›å»ºè§¦å‘å™¨ï¼Œä¿æŒåŒæ­¥
            await self._create_triggers(db)

            await db.commit()
            logger.info(f"FTS5 index initialized: {self.fts_table_name}")

    async def _create_triggers(self, db: aiosqlite.Connection):
        """åˆ›å»ºæ•°æ®åŒæ­¥è§¦å‘å™¨ï¼ˆå·²ç§»é™¤ - æ”¹ä¸ºæ‰‹åŠ¨æ’å…¥ä»¥æ”¯æŒé¢„å¤„ç†ï¼‰"""
        # æ³¨æ„ï¼šè§¦å‘å™¨å·²ç§»é™¤ï¼Œæ”¹ä¸ºåœ¨ SparseRetriever.add_document() ä¸­æ‰‹åŠ¨åŒæ­¥
        # è¿™æ ·å¯ä»¥åœ¨æ’å…¥å‰è¿›è¡Œåˆ†è¯å’Œåœç”¨è¯è¿‡æ»¤
        pass

    async def rebuild_index(self):
        """é‡å»ºç´¢å¼•"""
        async with aiosqlite.connect(self.db_path) as db:
            await db.execute(f"DELETE FROM {self.fts_table_name}")
            # æ³¨æ„ï¼šéœ€è¦æ‰‹åŠ¨åŒæ­¥ï¼Œå› ä¸ºè§¦å‘å™¨å·²ç§»é™¤
            # è¿™ä¸ªæ–¹æ³•åº”è¯¥ç”± SparseRetriever è°ƒç”¨
            await db.commit()
            logger.info("FTS index cleared (éœ€è¦æ‰‹åŠ¨é‡æ–°ç´¢å¼•æ‰€æœ‰æ–‡æ¡£)")

    def preprocess_text(self, text: str) -> str:
        """
        é¢„å¤„ç†æ–‡æœ¬ï¼šåˆ†è¯ + åœç”¨è¯è¿‡æ»¤

        Args:
            text: åŸå§‹æ–‡æœ¬

        Returns:
            str: é¢„å¤„ç†åçš„æ–‡æœ¬ï¼ˆç©ºæ ¼åˆ†éš”çš„tokensï¼‰
        """
        if not text or not text.strip():
            return ""

        # 1. ç§»é™¤å¤šä½™ç©ºç™½
        text = " ".join(text.split())

        # 2. ä¸­æ–‡åˆ†è¯
        if JIEBA_AVAILABLE:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡
            if any("\u4e00" <= char <= "\u9fff" for char in text):
                tokens = list(jieba.cut_for_search(text))
            else:
                # éä¸­æ–‡ï¼ŒæŒ‰ç©ºæ ¼åˆ†è¯
                tokens = text.split()
        else:
            tokens = text.split()

        # 3. å»é™¤åœç”¨è¯å’Œæ ‡ç‚¹
        if self.use_stopwords and self.stopwords_manager:
            filtered_tokens = []
            for token in tokens:
                # è·³è¿‡ç©ºtoken
                if not token or token.isspace():
                    continue
                # è·³è¿‡çº¯æ ‡ç‚¹
                if all(not c.isalnum() for c in token):
                    continue
                # è·³è¿‡åœç”¨è¯
                if not self.stopwords_manager.is_stopword(token):
                    filtered_tokens.append(token)
            tokens = filtered_tokens
        else:
            # å³ä½¿ä¸ç”¨åœç”¨è¯ï¼Œä¹Ÿè¦è¿‡æ»¤ç©ºç™½å’Œçº¯æ ‡ç‚¹
            tokens = [
                t
                for t in tokens
                if t and not t.isspace() and any(c.isalnum() for c in t)
            ]

        # 4. è¿”å›ç©ºæ ¼åˆ†éš”çš„tokens
        return " ".join(tokens)

    async def add_document(self, doc_id: int, content: str):
        """
        æ·»åŠ å•ä¸ªæ–‡æ¡£åˆ° FTS ç´¢å¼•

        Args:
            doc_id: æ–‡æ¡£ ID
            content: æ–‡æ¡£å†…å®¹
        """
        processed_content = self.preprocess_text(content)

        async with aiosqlite.connect(self.db_path) as db:
            await db.execute(
                f"INSERT INTO {self.fts_table_name}(doc_id, content) VALUES (?, ?)",
                (doc_id, processed_content),
            )
            await db.commit()
            logger.debug(
                f"FTSæ–‡æ¡£å·²æ·»åŠ : ID={doc_id}, åŸå§‹é•¿åº¦={len(content)}, å¤„ç†å={len(processed_content)}"
            )

    async def update_document(self, doc_id: int, content: str):
        """
        æ›´æ–°æ–‡æ¡£å†…å®¹

        Args:
            doc_id: æ–‡æ¡£ ID
            content: æ–°å†…å®¹
        """
        processed_content = self.preprocess_text(content)

        async with aiosqlite.connect(self.db_path) as db:
            await db.execute(
                f"DELETE FROM {self.fts_table_name} WHERE doc_id = ?", (doc_id,)
            )
            await db.execute(
                f"INSERT INTO {self.fts_table_name}(doc_id, content) VALUES (?, ?)",
                (doc_id, processed_content),
            )
            await db.commit()
            logger.debug(f"FTSæ–‡æ¡£å·²æ›´æ–°: ID={doc_id}")

    async def delete_document(self, doc_id: int):
        """
        åˆ é™¤æ–‡æ¡£

        Args:
            doc_id: æ–‡æ¡£ ID
        """
        async with aiosqlite.connect(self.db_path) as db:
            await db.execute(
                f"DELETE FROM {self.fts_table_name} WHERE doc_id = ?", (doc_id,)
            )
            await db.commit()
            logger.debug(f"FTSæ–‡æ¡£å·²åˆ é™¤: ID={doc_id}")

    async def search(self, query: str, limit: int = 50) -> List[Tuple[int, float]]:
        """æ‰§è¡Œ BM25 æœç´¢"""
        async with aiosqlite.connect(self.db_path) as db:
            # å°†æ•´ä¸ªæŸ¥è¯¢ç”¨åŒå¼•å·åŒ…è£¹ï¼Œä»¥å¤„ç†ç‰¹æ®Šå­—ç¬¦å¹¶å°†å…¶ä½œä¸ºçŸ­è¯­æœç´¢
            # è¿™æ˜¯ä¸ºäº†é˜²æ­¢ FTS5 è¯­æ³•é”™è¯¯ï¼Œä¾‹å¦‚ 'syntax error near "."'
            safe_query = f'"{query}"'

            # ä½¿ç”¨ BM25 ç®—æ³•æœç´¢
            cursor = await db.execute(
                f"""
                SELECT doc_id, bm25({self.fts_table_name}) as score 
                FROM {self.fts_table_name} 
                WHERE {self.fts_table_name} MATCH ?
                ORDER BY score
                LIMIT ?
            """,
                (safe_query, limit),
            )

            results = await cursor.fetchall()
            return [(row[0], row[1]) for row in results]


class SparseRetriever:
    """ç¨€ç–æ£€ç´¢å™¨"""

    def __init__(self, db_path: str, config: Dict[str, Any] = None):
        self.db_path = db_path
        self.config = config or {}
        self.enabled = self.config.get("enabled", True)
        self.use_chinese_tokenizer = self.config.get(
            "use_chinese_tokenizer", JIEBA_AVAILABLE
        )

        # åœç”¨è¯é…ç½®
        self.enable_stopwords = self.config.get("enable_stopwords_filtering", True)
        self.stopwords_source = self.config.get("stopwords_source", "hit")
        self.custom_stopwords = self.config.get("custom_stopwords", [])
        self.stopwords_manager: Optional[StopwordsManager] = None

        # åˆå§‹åŒ– FTS ç®¡ç†å™¨ï¼ˆç¨åä¼šè®¾ç½® stopwords_managerï¼‰
        self.fts_manager: Optional[FTSManager] = None

        logger.info("SparseRetriever åˆå§‹åŒ–")
        logger.info(f"  å¯ç”¨çŠ¶æ€: {'æ˜¯' if self.enabled else 'å¦'}")
        logger.info(
            f"  ä¸­æ–‡åˆ†è¯: {'æ˜¯' if self.use_chinese_tokenizer else 'å¦'} (jieba {'å¯ç”¨' if JIEBA_AVAILABLE else 'ä¸å¯ç”¨'})"
        )
        logger.info(f"  åœç”¨è¯è¿‡æ»¤: {'æ˜¯' if self.enable_stopwords else 'å¦'}")
        logger.info(f"  åœç”¨è¯æ¥æº: {self.stopwords_source}")
        logger.info(f"  è‡ªå®šä¹‰åœç”¨è¯: {len(self.custom_stopwords)} ä¸ª")
        logger.info(f"  æ•°æ®åº“è·¯å¾„: {db_path}")

    async def initialize(self):
        """åˆå§‹åŒ–ç¨€ç–æ£€ç´¢å™¨"""
        if not self.enabled:
            logger.info("ç¨€ç–æ£€ç´¢å™¨å·²ç¦ç”¨ï¼Œè·³è¿‡åˆå§‹åŒ–")
            return

        logger.info("å¼€å§‹åˆå§‹åŒ–ç¨€ç–æ£€ç´¢å™¨...")

        try:
            # 1. åˆå§‹åŒ–åœç”¨è¯ç®¡ç†å™¨
            if self.enable_stopwords:
                logger.info("åˆå§‹åŒ–åœç”¨è¯ç®¡ç†å™¨...")
                self.stopwords_manager = StopwordsManager()
                await self.stopwords_manager.load_stopwords(
                    source=self.stopwords_source,
                    custom_words=self.custom_stopwords,
                    auto_download=True,
                )
                logger.info(
                    f"âœ… åœç”¨è¯ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸï¼Œå…± {len(self.stopwords_manager.stopwords)} ä¸ªåœç”¨è¯"
                )
            else:
                logger.info("åœç”¨è¯è¿‡æ»¤å·²ç¦ç”¨")
                self.stopwords_manager = None

            # 2. åˆå§‹åŒ– FTS ç®¡ç†å™¨
            self.fts_manager = FTSManager(self.db_path, self.stopwords_manager)
            await self.fts_manager.initialize()
            logger.info("âœ… FTS5 ç´¢å¼•åˆå§‹åŒ–æˆåŠŸ")

        except Exception as e:
            logger.error(
                f"âŒ ç¨€ç–æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {type(e).__name__}: {e}", exc_info=True
            )
            raise

        # å¦‚æœå¯ç”¨ä¸­æ–‡åˆ†è¯ï¼Œåˆå§‹åŒ– jieba
        if self.use_chinese_tokenizer and JIEBA_AVAILABLE:
            logger.debug("jieba ä¸­æ–‡åˆ†è¯å·²å¯ç”¨")
            # å¯ä»¥æ·»åŠ è‡ªå®šä¹‰è¯å…¸
            pass

        logger.info("âœ… ç¨€ç–æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")

    def _preprocess_query(self, query: str) -> str:
        """
        é¢„å¤„ç†æŸ¥è¯¢ï¼ŒåŒ…æ‹¬åˆ†è¯å’Œå®‰å…¨è½¬ä¹‰ã€‚

        Args:
            query: åŸå§‹æŸ¥è¯¢å­—ç¬¦ä¸²

        Returns:
            str: å¤„ç†åçš„å®‰å…¨æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        if not query or not query.strip():
            return ""

        # ä½¿ç”¨ FTSManager çš„é¢„å¤„ç†æ–¹æ³•
        if self.fts_manager:
            processed = self.fts_manager.preprocess_text(query)
        else:
            processed = query.strip()

        # FTS5 å®‰å…¨è½¬ä¹‰: åŒå¼•å·éœ€è¦è½¬ä¹‰ä¸ºä¸¤ä¸ªåŒå¼•å·
        processed = processed.replace('"', '""')

        return processed

    async def search(
        self,
        query: str,
        limit: int = 50,
        session_id: Optional[str] = None,
        persona_id: Optional[str] = None,
        metadata_filters: Optional[Dict[str, Any]] = None,
    ) -> List[SparseResult]:
        """æ‰§è¡Œç¨€ç–æ£€ç´¢"""
        if not self.enabled:
            logger.debug("ç¨€ç–æ£€ç´¢å™¨æœªå¯ç”¨ï¼Œè¿”å›ç©ºç»“æœ")
            return []

        logger.debug(f"ç¨€ç–æ£€ç´¢: query='{query[:50]}...', limit={limit}")

        try:
            # é¢„å¤„ç†æŸ¥è¯¢
            processed_query = self._preprocess_query(query)
            logger.debug(f"  åŸå§‹æŸ¥è¯¢: '{query[:50]}...'")
            logger.debug(f"  å¤„ç†åæŸ¥è¯¢: '{processed_query[:50]}...'")

            # æ‰§è¡Œ FTS æœç´¢
            fts_results = await self.fts_manager.search(processed_query, limit)

            if not fts_results:
                logger.debug("  FTS æœç´¢æ— ç»“æœ")
                return []

            logger.debug(f"  FTS è¿”å› {len(fts_results)} æ¡åŸå§‹ç»“æœ")

            # è·å–å®Œæ•´çš„æ–‡æ¡£ä¿¡æ¯
            doc_ids = [doc_id for doc_id, _ in fts_results]
            logger.debug(f"  è·å–æ–‡æ¡£è¯¦æƒ…: {len(doc_ids)} ä¸ª ID")
            documents = await self._get_documents(doc_ids)
            logger.debug(f"  æˆåŠŸè·å– {len(documents)} ä¸ªæ–‡æ¡£")

            # åº”ç”¨è¿‡æ»¤å™¨
            filtered_results = []
            for doc_id, bm25_score in fts_results:
                if doc_id in documents:
                    doc = documents[doc_id]

                    # æ£€æŸ¥å…ƒæ•°æ®è¿‡æ»¤å™¨
                    if self._apply_filters(
                        doc.get("metadata", {}),
                        session_id,
                        persona_id,
                        metadata_filters,
                    ):
                        result = SparseResult(
                            doc_id=doc_id,
                            score=bm25_score,
                            content=doc["text"],
                            metadata=doc["metadata"],
                        )
                        filtered_results.append(result)

            logger.debug(f"  è¿‡æ»¤åå‰©ä½™ {len(filtered_results)} æ¡ç»“æœ")

            # å½’ä¸€åŒ– BM25 åˆ†æ•°ï¼ˆè½¬æ¢ä¸º 0-1ï¼‰
            if filtered_results:
                max_score = max(r.score for r in filtered_results)
                min_score = min(r.score for r in filtered_results)
                score_range = max_score - min_score if max_score != min_score else 1

                logger.debug(
                    f"  å½’ä¸€åŒ–åˆ†æ•°: min={min_score:.3f}, max={max_score:.3f}, range={score_range:.3f}"
                )

                for result in filtered_results:
                    original_score = result.score
                    result.score = (result.score - min_score) / score_range
                    logger.debug(
                        f"    ID={result.doc_id}: {original_score:.3f} -> {result.score:.3f}"
                    )

            logger.info(f"âœ… ç¨€ç–æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(filtered_results)} æ¡ç»“æœ")
            return filtered_results

        except Exception as e:
            logger.error(f"âŒ ç¨€ç–æ£€ç´¢å¤±è´¥: {type(e).__name__}: {e}", exc_info=True)
            logger.error(f"  å¤±è´¥ä¸Šä¸‹æ–‡: query='{query[:50]}...', limit={limit}")
            return []

    async def _get_documents(self, doc_ids: List[int]) -> Dict[int, Dict[str, Any]]:
        """æ‰¹é‡è·å–æ–‡æ¡£"""
        async with aiosqlite.connect(self.db_path) as db:
            placeholders = ",".join("?" for _ in doc_ids)
            cursor = await db.execute(
                f"""
                SELECT id, text, metadata FROM documents WHERE id IN ({placeholders})
            """,
                doc_ids,
            )

            documents = {}
            async for row in cursor:
                metadata = json.loads(row[2]) if isinstance(row[2], str) else row[2]
                documents[row[0]] = {"text": row[1], "metadata": metadata or {}}

            return documents

    def _apply_filters(
        self,
        metadata: Dict[str, Any],
        session_id: Optional[str],
        persona_id: Optional[str],
        metadata_filters: Optional[Dict[str, Any]],
    ) -> bool:
        """åº”ç”¨è¿‡æ»¤å™¨"""
        # ä¼šè¯è¿‡æ»¤
        if session_id and metadata.get("session_id") != session_id:
            return False

        # äººæ ¼è¿‡æ»¤
        if persona_id and metadata.get("persona_id") != persona_id:
            return False

        # è‡ªå®šä¹‰å…ƒæ•°æ®è¿‡æ»¤
        if metadata_filters:
            for key, value in metadata_filters.items():
                if metadata.get(key) != value:
                    return False

        return True

    async def add_document(self, doc_id: int, content: str):
        """
        æ·»åŠ æ–‡æ¡£åˆ° FTS ç´¢å¼•

        Args:
            doc_id: æ–‡æ¡£ ID
            content: æ–‡æ¡£å†…å®¹
        """
        if not self.enabled or not self.fts_manager:
            return

        await self.fts_manager.add_document(doc_id, content)

    async def update_document(self, doc_id: int, content: str):
        """
        æ›´æ–° FTS ç´¢å¼•ä¸­çš„æ–‡æ¡£

        Args:
            doc_id: æ–‡æ¡£ ID
            content: æ–°å†…å®¹
        """
        if not self.enabled or not self.fts_manager:
            return

        await self.fts_manager.update_document(doc_id, content)

    async def delete_document(self, doc_id: int):
        """
        ä» FTS ç´¢å¼•åˆ é™¤æ–‡æ¡£

        Args:
            doc_id: æ–‡æ¡£ ID
        """
        if not self.enabled or not self.fts_manager:
            return

        await self.fts_manager.delete_document(doc_id)

    async def rebuild_index(self):
        """é‡å»ºç´¢å¼•ï¼ˆä» documents è¡¨åŒæ­¥æ‰€æœ‰æ•°æ®ï¼‰"""
        if not self.enabled:
            logger.warning("ç¨€ç–æ£€ç´¢å™¨æœªå¯ç”¨ï¼Œæ— æ³•é‡å»ºç´¢å¼•")
            return

        logger.info("ğŸ”„ å¼€å§‹é‡å»º FTS5 ç´¢å¼•...")

        try:
            # 1. æ¸…ç©ºç°æœ‰ç´¢å¼•
            await self.fts_manager.rebuild_index()

            # 2. ä» documents è¡¨é‡æ–°åŠ è½½æ‰€æœ‰æ•°æ®
            async with aiosqlite.connect(self.db_path) as db:
                cursor = await db.execute("SELECT id, text FROM documents")
                rows = await cursor.fetchall()

                logger.info(f"æ‰¾åˆ° {len(rows)} ä¸ªæ–‡æ¡£éœ€è¦ç´¢å¼•")

                # 3. æ‰¹é‡æ·»åŠ æ–‡æ¡£
                for row in rows:
                    doc_id, content = row
                    await self.fts_manager.add_document(doc_id, content)

            logger.info(f"âœ… FTS5 ç´¢å¼•é‡å»ºæˆåŠŸï¼Œå·²ç´¢å¼• {len(rows)} ä¸ªæ–‡æ¡£")

        except Exception as e:
            logger.error(
                f"âŒ é‡å»º FTS5 ç´¢å¼•å¤±è´¥: {type(e).__name__}: {e}", exc_info=True
            )
            raise
