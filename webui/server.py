# -*- coding: utf-8 -*-
"""
server.py - LivingMemory WebUI backend
Provides authentication, memory browsing, detail view and bulk deletion APIs built on FastAPI.

WebUI åŠŸèƒ½åˆ—è¡¨:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ è®°å¿†ç®¡ç†:
  - æŸ¥çœ‹è®°å¿†åˆ—è¡¨ï¼ˆåˆ†é¡µã€ç­›é€‰ã€æœç´¢ï¼‰
  - æŸ¥çœ‹è®°å¿†è¯¦æƒ…ï¼ˆå®Œæ•´å…ƒæ•°æ®å’ŒJSONï¼‰
  - ç¼–è¾‘è®°å¿†ï¼ˆå†…å®¹ã€é‡è¦æ€§ã€ç±»å‹ã€çŠ¶æ€ï¼‰
  - åˆ é™¤è®°å¿†ï¼ˆå•ä¸ªæˆ–æ‰¹é‡åˆ é™¤ï¼‰
  - æ ¸çˆ†æ¸…é™¤ï¼ˆå»¶è¿Ÿç¡®è®¤æœºåˆ¶ï¼‰

âš™ï¸ ç³»ç»Ÿç®¡ç†:
  - è§¦å‘é—å¿˜ä»£ç†ï¼ˆæ‰‹åŠ¨æ¸…ç†ä½é‡è¦æ€§è®°å¿†ï¼‰
  - é‡å»ºç¨€ç–ç´¢å¼•ï¼ˆBM25ç´¢å¼•é‡å»ºï¼‰
  - ä¼šè¯ç®¡ç†ï¼ˆæŸ¥çœ‹æ´»è·ƒä¼šè¯åˆ—è¡¨ï¼‰

ğŸ› ï¸ è°ƒè¯•å·¥å…·:
  - æ£€ç´¢æµ‹è¯•ï¼ˆæµ‹è¯•Dense/Sparse/Hybridæ¨¡å¼ï¼‰
  - èåˆç­–ç•¥å¯¹æ¯”ï¼ˆå¯¹æ¯”å¤šç§èåˆç®—æ³•æ•ˆæœï¼‰
  - è®°å¿†ç»Ÿè®¡åˆ†æï¼ˆåˆ†å¸ƒç»Ÿè®¡å’Œè¶‹åŠ¿åˆ†æï¼‰

ğŸ“Š æ•°æ®å±•ç¤º:
  - å®æ—¶ç»Ÿè®¡ï¼ˆæ€»è®°å¿†æ•°ã€çŠ¶æ€åˆ†å¸ƒã€æ´»è·ƒä¼šè¯ï¼‰
  - åˆ†é¡µæµè§ˆï¼ˆæ”¯æŒè‡ªå®šä¹‰æ¯é¡µæ•°é‡ï¼‰
  - å…³é”®è¯æœç´¢ï¼ˆæ”¯æŒmemory_idå’Œå†…å®¹æœç´¢ï¼‰
  - çŠ¶æ€ç­›é€‰ï¼ˆæ´»è·ƒ/å·²å½’æ¡£/å·²åˆ é™¤ï¼‰

ğŸ” å®‰å…¨ç‰¹æ€§:
  - å¯†ç è®¤è¯ï¼ˆå…¥å£å¯†ç ä¿æŠ¤ï¼‰
  - ä¼šè¯ç®¡ç†ï¼ˆè‡ªåŠ¨è¶…æ—¶å’Œæ¸…ç†ï¼‰
  - è¯·æ±‚é¢‘ç‡é™åˆ¶ï¼ˆé˜²æš´åŠ›ç ´è§£ï¼‰
  - Tokenæœ‰æ•ˆæœŸç®¡ç†ï¼ˆç»å¯¹è¿‡æœŸ+æ´»åŠ¨è¶…æ—¶ï¼‰

APIç«¯ç‚¹è¯´æ˜:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
è®¤è¯ç›¸å…³:
  POST   /api/login                    - ç”¨æˆ·ç™»å½•
  POST   /api/logout                   - ç”¨æˆ·ç™»å‡º
  GET    /api/health                   - å¥åº·æ£€æŸ¥

è®°å¿†ç®¡ç†:
  GET    /api/memories                 - è·å–è®°å¿†åˆ—è¡¨ï¼ˆæ”¯æŒåˆ†é¡µã€ç­›é€‰ã€æœç´¢ï¼‰
  GET    /api/memories/{memory_id}     - è·å–è®°å¿†è¯¦æƒ…
  PUT    /api/memories/{memory_id}     - æ›´æ–°è®°å¿†å­—æ®µ
  DELETE /api/memories                 - æ‰¹é‡åˆ é™¤è®°å¿†
  POST   /api/memories/nuke            - è°ƒåº¦æ ¸çˆ†ä»»åŠ¡
  GET    /api/memories/nuke            - è·å–æ ¸çˆ†çŠ¶æ€
  DELETE /api/memories/nuke/{op_id}    - å–æ¶ˆæ ¸çˆ†ä»»åŠ¡

ç³»ç»Ÿç®¡ç†:
  GET    /api/stats                           - è·å–ç»Ÿè®¡ä¿¡æ¯
  POST   /api/admin/forgetting-agent/trigger  - æ‰‹åŠ¨è§¦å‘é—å¿˜ä»£ç†
  POST   /api/admin/sparse-index/rebuild      - é‡å»ºç¨€ç–ç´¢å¼•
  GET    /api/admin/sessions                  - è·å–ä¼šè¯åˆ—è¡¨

è°ƒè¯•å·¥å…·:
  POST   /api/debug/search-test          - æµ‹è¯•æ£€ç´¢åŠŸèƒ½
  POST   /api/debug/fusion-comparison    - å¯¹æ¯”èåˆç­–ç•¥
  GET    /api/debug/memory-analysis      - åˆ†æè®°å¿†ç»Ÿè®¡

æ³¨æ„äº‹é¡¹:
  - æ‰€æœ‰APIï¼ˆé™¤loginå’Œhealthï¼‰éƒ½éœ€è¦Bearer Tokenè®¤è¯
  - Tokenæœ‰24å°æ—¶ç»å¯¹è¿‡æœŸæ—¶é—´å’Œå¯é…ç½®çš„æ´»åŠ¨è¶…æ—¶æ—¶é—´
  - æ ¸çˆ†åŠŸèƒ½ä»…ä¸ºè§†è§‰æ•ˆæœï¼Œä¸ä¼šçœŸå®åˆ é™¤æ•°æ®ï¼ˆä¿æŠ¤ç”¨æˆ·æ•°æ®å®‰å…¨ï¼‰
  - ç¼–è¾‘è®°å¿†ä¼šè‡ªåŠ¨è®°å½•æ›´æ–°å†å²ï¼ˆæ—¶é—´ã€å­—æ®µã€åŸå€¼ã€æ–°å€¼ã€åŸå› ï¼‰
"""


import asyncio
import json
import secrets
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional, Tuple,List, TYPE_CHECKING

import uvicorn
from fastapi import Depends, FastAPI, HTTPException, Request, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles

from astrbot.api import logger

from ..core.utils import safe_parse_metadata
from ..storage.memory_storage import MemoryStorage

if TYPE_CHECKING:
    from ..storage.faiss_manager import FaissManager
    from ..main import SessionManager
    from ..core.engines.recall_engine import RecallEngine
    from ..core.engines.reflection_engine import ReflectionEngine
    from ..core.engines.forgetting_agent import ForgettingAgent
    from ..core.retrieval import SparseRetriever


class WebUIServer:
    """
    Helper class responsible for starting and managing the LivingMemory WebUI service.
    """

    def __init__(
        self,
        config: Dict[str, Any],
        faiss_manager: "FaissManager",
        session_manager: Optional["SessionManager"] = None,
        recall_engine: Optional["RecallEngine"] = None,
        reflection_engine: Optional["ReflectionEngine"] = None,
        forgetting_agent: Optional["ForgettingAgent"] = None,
        sparse_retriever: Optional["SparseRetriever"] = None,
    ):
        self.config = config
        self.faiss_manager = faiss_manager
        self.session_manager = session_manager
        self.recall_engine = recall_engine
        self.reflection_engine = reflection_engine
        self.forgetting_agent = forgetting_agent
        self.sparse_retriever = sparse_retriever

        self.host = str(config.get("host", "127.0.0.1"))
        self.port = int(config.get("port", 8080))
        self.session_timeout = max(60, int(config.get("session_timeout", 3600)))
        self._access_password = str(config.get("access_password", "")).strip()

        # Token ç®¡ç† - ä¿®å¤: ä½¿ç”¨å­—å…¸å­˜å‚¨æ›´å¤šä¿¡æ¯é˜²æ­¢æ°¸ä¸è¿‡æœŸ
        self._tokens: Dict[str, Dict[str, float]] = {}
        self._token_lock = asyncio.Lock()

        # è¯·æ±‚é¢‘ç‡é™åˆ¶ - æ–°å¢: é˜²æ­¢æš´åŠ›ç ´è§£
        self._failed_attempts: Dict[str, List[float]] = {}
        self._attempt_lock = asyncio.Lock()

        self._server: Optional[uvicorn.Server] = None
        self._server_task: Optional[asyncio.Task] = None
        self._cleanup_task: Optional[asyncio.Task] = None
        self._nuke_task: Optional[asyncio.Task] = None

        self.memory_storage: Optional[MemoryStorage] = None
        self._storage_prepared = False
        self._pending_nuke: Optional[Dict[str, Any]] = None
        self._nuke_lock = asyncio.Lock()

        self._app = FastAPI(title="LivingMemory æ§åˆ¶å°", version="1.3.3")
        self._setup_routes()

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    async def start(self):
        """
        å¯åŠ¨ WebUI æœåŠ¡ã€‚
        """
        if self._server_task and not self._server_task.done():
            logger.warning("WebUI æœåŠ¡å·²ç»åœ¨è¿è¡Œ")
            return

        await self._prepare_storage()

        config = uvicorn.Config(
            app=self._app,
            host=self.host,
            port=self.port,
            log_level="info",
            loop="asyncio",
            lifespan="on",
        )
        self._server = uvicorn.Server(config)
        self._server_task = asyncio.create_task(self._server.serve())

        # å¯åŠ¨å®šæœŸæ¸…ç†ä»»åŠ¡ - æ–°å¢: é˜²æ­¢å†…å­˜æ³„æ¼
        self._cleanup_task = asyncio.create_task(self._periodic_cleanup())

        # ç­‰å¾…æœåŠ¡å¯åŠ¨
        for _ in range(50):
            if getattr(self._server, "started", False):
                logger.info(f"WebUI å·²å¯åŠ¨: http://{self.host}:{self.port}")
                return
            if self._server_task.done():
                error = self._server_task.exception()
                raise RuntimeError(f"WebUI å¯åŠ¨å¤±è´¥: {error}") from error
            await asyncio.sleep(0.1)

        logger.warning("WebUI å¯åŠ¨è€—æ—¶è¾ƒé•¿ï¼Œä»åœ¨åå°å¯åŠ¨ä¸­")

    async def stop(self):
        """
        åœæ­¢ WebUI æœåŠ¡ã€‚
        """
        # åœæ­¢å®šæœŸæ¸…ç†ä»»åŠ¡
        if self._cleanup_task and not self._cleanup_task.done():
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass

        if self._server:
            self._server.should_exit = True
        if self._server_task:
            await self._server_task
        self._server = None
        self._server_task = None
        if self._nuke_task and not self._nuke_task.done():
            self._nuke_task.cancel()
            try:
                await self._nuke_task
            except asyncio.CancelledError:
                pass
        self._nuke_task = None
        self._pending_nuke = None
        self._cleanup_task = None
        logger.info("WebUI å·²åœæ­¢")

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------

    async def _periodic_cleanup(self):
        """
        å®šæœŸæ¸…ç†è¿‡æœŸ token å’Œå¤±è´¥å°è¯•è®°å½• - æ–°å¢: é˜²æ­¢å†…å­˜æ³„æ¼
        """
        while True:
            try:
                await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
                async with self._token_lock:
                    await self._cleanup_tokens_locked()
                async with self._attempt_lock:
                    await self._cleanup_failed_attempts_locked()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"å®šæœŸæ¸…ç†ä»»åŠ¡å‡ºé”™: {e}")

    async def _cleanup_failed_attempts_locked(self):
        """
        æ¸…ç†è¿‡æœŸçš„å¤±è´¥å°è¯•è®°å½• - æ–°å¢
        """
        now = time.time()
        expired_ips = []
        for ip, attempts in self._failed_attempts.items():
            # åªä¿ç•™5åˆ†é’Ÿå†…çš„å°è¯•è®°å½•
            recent = [t for t in attempts if now - t < 300]
            if recent:
                self._failed_attempts[ip] = recent
            else:
                expired_ips.append(ip)

        for ip in expired_ips:
            self._failed_attempts.pop(ip, None)

    async def _check_rate_limit(self, client_ip: str) -> bool:
        """
        æ£€æŸ¥è¯·æ±‚é¢‘ç‡é™åˆ¶ - æ–°å¢: é˜²æ­¢æš´åŠ›ç ´è§£

        Returns:
            bool: True è¡¨ç¤ºæœªè¶…é™, False è¡¨ç¤ºå·²è¶…é™
        """
        async with self._attempt_lock:
            await self._cleanup_failed_attempts_locked()
            attempts = self._failed_attempts.get(client_ip, [])
            recent = [t for t in attempts if time.time() - t < 300]  # 5åˆ†é’Ÿçª—å£

            if len(recent) >= 5:  # 5åˆ†é’Ÿå†…æœ€å¤š5æ¬¡å¤±è´¥å°è¯•
                return False
            return True

    async def _record_failed_attempt(self, client_ip: str):
        """
        è®°å½•å¤±è´¥çš„ç™»å½•å°è¯• - æ–°å¢
        """
        async with self._attempt_lock:
            if client_ip not in self._failed_attempts:
                self._failed_attempts[client_ip] = []
            self._failed_attempts[client_ip].append(time.time())

    async def _prepare_storage(self):
        """
        åˆå§‹åŒ–è‡ªå®šä¹‰è®°å¿†å­˜å‚¨ï¼ˆå¦‚å¯ç”¨ï¼‰ã€‚
        """
        if self._storage_prepared:
            return

        connection = None
        try:
            doc_storage = getattr(self.faiss_manager.db, "document_storage", None)
            connection = getattr(doc_storage, "connection", None)
        except Exception as exc:  # pragma: no cover
            logger.debug(f"è·å–æ–‡æ¡£å­˜å‚¨è¿æ¥å¤±è´¥: {exc}")

        if connection:
            try:
                storage = MemoryStorage(connection)
                await storage.initialize_schema()
                self.memory_storage = storage
                logger.info("WebUI å·²æ¥å…¥æ’ä»¶è‡ªå®šä¹‰çš„è®°å¿†å­˜å‚¨ï¼ˆSQLiteï¼‰")
            except Exception as exc:
                logger.warning(f"åˆå§‹åŒ– MemoryStorage å¤±è´¥ï¼Œå°†å›é€€è‡³æ–‡æ¡£å­˜å‚¨: {exc}")
                self.memory_storage = None
        else:
            logger.debug("æœªè·å–åˆ° MemoryStorage è¿æ¥ï¼Œå°†ä»…ä½¿ç”¨ Faiss æ–‡æ¡£å­˜å‚¨æ¥å£")

        self._storage_prepared = True

    def _setup_routes(self):
        """
        åˆå§‹åŒ– FastAPI è·¯ç”±ä¸é™æ€èµ„æºã€‚
        """
        static_dir = Path(__file__).resolve().parent.parent / "static"
        index_path = static_dir / "index.html"
        if not index_path.exists():
            logger.warning("æœªæ‰¾åˆ° WebUI å‰ç«¯æ–‡ä»¶ï¼Œé™æ€èµ„æºç›®å½•ä¸ºç©º")

        self._app.add_middleware(
            CORSMiddleware,
            allow_origins=[
                f"http://{self.host}:{self.port}",
                "http://localhost",
                "http://127.0.0.1",
            ],
            allow_methods=["GET", "POST", "PUT", "DELETE"],  # æ·»åŠ  PUT æ–¹æ³•
            allow_headers=["Content-Type", "Authorization", "X-Auth-Token"],
            allow_credentials=True,
        )

        self._app.mount("/static", StaticFiles(directory=static_dir), name="static")

        @self._app.get("/", response_class=HTMLResponse)
        async def serve_index():
            if not index_path.exists():
                raise HTTPException(status.HTTP_404_NOT_FOUND, detail="å‰ç«¯æ–‡ä»¶ç¼ºå¤±")
            return HTMLResponse(index_path.read_text(encoding="utf-8"))

        @self._app.post("/api/login")
        async def login(request: Request, payload: Dict[str, Any]):
            password = str(payload.get("password", "")).strip()
            if not password:
                raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="å¯†ç ä¸èƒ½ä¸ºç©º")

            # æ£€æŸ¥è¯·æ±‚é¢‘ç‡é™åˆ¶ - æ–°å¢
            client_ip = request.client.host if request.client else "unknown"
            if not await self._check_rate_limit(client_ip):
                raise HTTPException(
                    status.HTTP_429_TOO_MANY_REQUESTS,
                    detail="å°è¯•æ¬¡æ•°è¿‡å¤šï¼Œè¯·5åˆ†é’Ÿåå†è¯•"
                )

            if password != self._access_password:
                # è®°å½•å¤±è´¥å°è¯• - æ–°å¢
                await self._record_failed_attempt(client_ip)
                await asyncio.sleep(1.0)  # å¢åŠ å»¶è¿Ÿåˆ°1ç§’ï¼Œå‡ç¼“æš´åŠ›ç ´è§£
                raise HTTPException(status.HTTP_401_UNAUTHORIZED, detail="è®¤è¯å¤±è´¥")

            # ç”Ÿæˆ token - ä¿®å¤: ä½¿ç”¨å­—å…¸å­˜å‚¨å¤šä¸ªæ—¶é—´æˆ³
            token = secrets.token_urlsafe(32)
            now = time.time()
            max_lifetime = 86400  # 24å°æ—¶ç»å¯¹è¿‡æœŸ

            async with self._token_lock:
                await self._cleanup_tokens_locked()
                self._tokens[token] = {
                    "created_at": now,
                    "last_active": now,
                    "max_lifetime": max_lifetime
                }

            return {"token": token, "expires_in": self.session_timeout}

        @self._app.post("/api/logout")
        async def logout(token: str = Depends(self._auth_dependency())):
            async with self._token_lock:
                self._tokens.pop(token, None)
            return {"detail": "å·²é€€å‡ºç™»å½•"}

        @self._app.get("/api/memories")
        async def list_memories(
            request: Request,
            token: str = Depends(self._auth_dependency()),
        ):
            query = request.query_params
            keyword = query.get("keyword", "").strip()
            status_filter = query.get("status", "all").strip() or "all"
            load_all = query.get("all", "false").lower() == "true"

            if load_all:
                page = 1
                page_size = 0
                offset = 0
            else:
                page = max(1, int(query.get("page", 1)))
                page_size = query.get("page_size")
                page_size = min(200, max(1, int(page_size))) if page_size else 50
                offset = (page - 1) * page_size

            try:
                total, items = await self._fetch_memories(
                    page=page,
                    page_size=page_size,
                    offset=offset,
                    status_filter=status_filter,
                    keyword=keyword,
                    load_all=load_all,
                )
            except Exception as exc:
                logger.error(f"è·å–è®°å¿†åˆ—è¡¨å¤±è´¥: {exc}", exc_info=True)
                raise HTTPException(
                    status.HTTP_500_INTERNAL_SERVER_ERROR, detail="è¯»å–è®°å¿†å¤±è´¥"
                ) from exc

            has_more = False if load_all else offset + len(items) < total
            effective_page_size = page_size if page_size else len(items)

            return {
                "items": items,
                "page": page,
                "page_size": effective_page_size,
                "total": total,
                "has_more": has_more,
            }

        @self._app.get("/api/memories/{memory_id}")
        async def memory_detail(
            memory_id: str, token: str = Depends(self._auth_dependency())
        ):
            detail = await self._get_memory_detail(memory_id)
            if not detail:
                raise HTTPException(status.HTTP_404_NOT_FOUND, detail="æœªæ‰¾åˆ°è®°å¿†è®°å½•")
            return detail

        @self._app.delete("/api/memories")
        async def delete_memories(
            payload: Dict[str, Any],
            token: str = Depends(self._auth_dependency()),
        ):
            doc_ids = payload.get("doc_ids") or payload.get("ids") or []
            memory_ids = payload.get("memory_ids") or []

            if not doc_ids and not memory_ids:
                raise HTTPException(
                    status.HTTP_400_BAD_REQUEST, detail="éœ€è¦æä¾›å¾…åˆ é™¤çš„è®°å¿†IDåˆ—è¡¨"
                )

            deleted_docs = 0
            deleted_memories = 0

            if doc_ids:
                try:
                    doc_ids_int = [int(x) for x in doc_ids]
                    await self.faiss_manager.delete_memories(doc_ids_int)
                    deleted_docs = len(doc_ids_int)
                except Exception as exc:
                    logger.error(f"åˆ é™¤ Faiss è®°å¿†å¤±è´¥: {exc}", exc_info=True)
                    raise HTTPException(
                        status.HTTP_500_INTERNAL_SERVER_ERROR, detail="å‘é‡è®°å¿†åˆ é™¤å¤±è´¥"
                    ) from exc

            if memory_ids and self.memory_storage:
                try:
                    ids = [str(x) for x in memory_ids]
                    await self.memory_storage.delete_memories_by_memory_ids(ids)
                    deleted_memories = len(ids)
                except Exception as exc:
                    logger.error(f"åˆ é™¤ç»“æ„åŒ–è®°å¿†å¤±è´¥: {exc}", exc_info=True)
                    raise HTTPException(
                        status.HTTP_500_INTERNAL_SERVER_ERROR, detail="ç»“æ„åŒ–è®°å¿†åˆ é™¤å¤±è´¥"
                    ) from exc

            return {
                "deleted_doc_count": deleted_docs,
                "deleted_memory_count": deleted_memories,
            }

        @self._app.post("/api/memories/nuke")
        async def schedule_memory_nuke(
            payload: Optional[Dict[str, Any]] = None,
            token: str = Depends(self._auth_dependency()),
        ):
            delay = 30
            if payload and "delay" in payload:
                try:
                    delay = int(payload["delay"])
                except (TypeError, ValueError):
                    raise HTTPException(
                        status.HTTP_400_BAD_REQUEST, detail="delay å‚æ•°æ— æ•ˆ"
                    )
            return await self._schedule_nuke(delay)

        @self._app.get("/api/memories/nuke")
        async def get_memory_nuke_status(
            token: str = Depends(self._auth_dependency()),
        ):
            return await self._get_pending_nuke()

        @self._app.delete("/api/memories/nuke/{operation_id}")
        async def cancel_memory_nuke(
            operation_id: str,
            token: str = Depends(self._auth_dependency()),
        ):
            cancelled = await self._cancel_nuke(operation_id)
            if not cancelled:
                raise HTTPException(
                    status.HTTP_404_NOT_FOUND, detail="å½“å‰æ²¡æœ‰åŒ¹é…çš„æ ¸çˆ†ä»»åŠ¡"
                )
            return {"detail": "å·²å–æ¶ˆæ ¸çˆ†ä»»åŠ¡", "operation_id": operation_id}

        @self._app.get("/api/stats")
        async def stats(token: str = Depends(self._auth_dependency())):
            total, status_counts = await self._gather_statistics()
            active_sessions = (
                self.session_manager.get_session_count()
                if self.session_manager
                else 0
            )

            return {
                "total_memories": total,
                "status_breakdown": status_counts,
                "active_sessions": active_sessions,
                "session_timeout": self.session_timeout,
            }

        @self._app.get("/api/health")
        async def health():
            return {"status": "ok"}

        # ========== è®°å¿†ç¼–è¾‘ API ==========
        @self._app.put("/api/memories/{memory_id}")
        async def update_memory(
            memory_id: str,
            payload: Dict[str, Any],
            token: str = Depends(self._auth_dependency()),
        ):
            """æ›´æ–°è®°å¿†å­—æ®µ"""
            return await self._update_memory_field(memory_id, payload)

        # ========== ç³»ç»Ÿç®¡ç† API ==========
        @self._app.post("/api/admin/forgetting-agent/trigger")
        async def trigger_forgetting_agent(
            token: str = Depends(self._auth_dependency()),
        ):
            """æ‰‹åŠ¨è§¦å‘é—å¿˜ä»£ç†"""
            return await self._trigger_forgetting_agent()

        @self._app.post("/api/admin/sparse-index/rebuild")
        async def rebuild_sparse_index(
            token: str = Depends(self._auth_dependency()),
        ):
            """é‡å»ºç¨€ç–ç´¢å¼•"""
            return await self._rebuild_sparse_index()

        @self._app.get("/api/admin/sessions")
        async def list_sessions(
            token: str = Depends(self._auth_dependency()),
        ):
            """è·å–ä¼šè¯åˆ—è¡¨"""
            return await self._get_sessions_info()

        # ========== è°ƒè¯•å·¥å…· API ==========
        @self._app.post("/api/debug/search-test")
        async def test_search(
            payload: Dict[str, Any],
            token: str = Depends(self._auth_dependency()),
        ):
            """æµ‹è¯•æ£€ç´¢åŠŸèƒ½"""
            return await self._test_search(payload)

        @self._app.post("/api/debug/fusion-comparison")
        async def compare_fusion_strategies(
            payload: Dict[str, Any],
            token: str = Depends(self._auth_dependency()),
        ):
            """å¯¹æ¯”èåˆç­–ç•¥"""
            return await self._compare_fusion_strategies(payload)

        @self._app.get("/api/debug/memory-analysis")
        async def analyze_memories(
            token: str = Depends(self._auth_dependency()),
        ):
            """åˆ†æè®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
            return await self._analyze_memories()

    async def _fetch_memories(
        self,
        page: int,
        page_size: int,
        offset: int,
        status_filter: str,
        keyword: str,
        load_all: bool,
    ) -> Tuple[int, list]:
        try:
            total, records = await self._query_faiss_memories(
                offset=offset,
                page_size=page_size,
                status_filter=status_filter,
                keyword=keyword,
                load_all=load_all,
            )
        except Exception as exc:
            logger.error(f"ä½¿ç”¨ä¼˜åŒ–æŸ¥è¯¢è·å–è®°å¿†å¤±è´¥ï¼Œå°†å›é€€åŸºç¡€å®ç°: {exc}", exc_info=True)
            total, records = await self._fetch_memories_fallback(
                offset=offset,
                page_size=page_size,
                status_filter=status_filter,
                keyword=keyword,
                load_all=load_all,
            )

        items = [self._format_memory(record, source="faiss") for record in records]
        return total, items

    async def _query_faiss_memories(
        self,
        offset: int,
        page_size: int,
        status_filter: str,
        keyword: str,
        load_all: bool,
    ) -> Tuple[int, List[Dict[str, Any]]]:
        doc_storage = getattr(self.faiss_manager.db, "document_storage", None)
        connection = getattr(doc_storage, "connection", None)
        if connection is None:
            raise RuntimeError("Document storage connection unavailable")

        conditions: List[str] = []
        params: List[Any] = []

        status_value = (status_filter or "").strip().lower()
        if status_value and status_value != "all":
            conditions.append("LOWER(COALESCE(json_extract(metadata, '$.status'), 'active')) = ?")
            params.append(status_value)

        keyword_value = (keyword or "").strip()
        if keyword_value:
            keyword_param = f"%{keyword_value.lower()}%"
            conditions.append("("
                "LOWER(text) LIKE ? OR "
                "LOWER(COALESCE(json_extract(metadata, '$.memory_content'), '')) LIKE ? OR "
                "LOWER(COALESCE(json_extract(metadata, '$.memory_id'), '')) LIKE ?"
                ")")
            params.extend([keyword_param, keyword_param, keyword_param])

        where_clause = f" WHERE {' AND '.join(conditions)}" if conditions else ""

        count_sql = f"SELECT COUNT(*) FROM documents{where_clause}"
        async with connection.execute(count_sql, params) as cursor:
            row = await cursor.fetchone()
        total = int(row[0]) if row and row[0] is not None else 0

        query_sql = (
            "SELECT id, text, metadata FROM documents"
            f"{where_clause} ORDER BY id DESC"
        )
        query_params = list(params)
        if not load_all and page_size > 0:
            query_sql += " LIMIT ? OFFSET ?"
            query_params.extend([page_size, offset])

        async with connection.execute(query_sql, query_params) as cursor:
            rows = await cursor.fetchall()

        records: List[Dict[str, Any]] = []
        for row in rows:
            metadata_raw = row[2]
            if isinstance(metadata_raw, str):
                try:
                    metadata = json.loads(metadata_raw)
                except json.JSONDecodeError:
                    metadata = {}
            else:
                metadata = metadata_raw or {}

            records.append(
                {
                    "id": row[0],
                    "content": row[1],
                    "metadata": metadata,
                }
            )

        return total, records

    async def _fetch_memories_fallback(
        self,
        offset: int,
        page_size: int,
        status_filter: str,
        keyword: str,
        load_all: bool,
    ) -> Tuple[int, List[Dict[str, Any]]]:
        total_available = await self.faiss_manager.count_total_memories()
        fetch_size = max(total_available, page_size if page_size else 0, 1)

        records = await self.faiss_manager.get_memories_paginated(
            page_size=fetch_size, offset=0
        )

        filtered_records = self._filter_records(records, status_filter, keyword)
        total_filtered = len(filtered_records)

        if load_all:
            return total_filtered, filtered_records

        start = max(0, offset)
        end = start + page_size if page_size else total_filtered
        return total_filtered, filtered_records[start:end]

    def _filter_records(
        self,
        records: List[Dict[str, Any]],
        status_filter: str,
        keyword: str
    ) -> List[Dict[str, Any]]:
        """
        åœ¨å†…å­˜ä¸­è¿‡æ»¤è®°å½• - æ–°å¢: æ”¯æŒçŠ¶æ€å’Œå…³é”®è¯ç­›é€‰
        """
        filtered = []

        for record in records:
            # metadata ç°åœ¨å·²ç»æ˜¯å­—å…¸
            metadata = record.get("metadata", {})

            # çŠ¶æ€è¿‡æ»¤
            if status_filter and status_filter != "all":
                record_status = metadata.get("status", "active")
                if record_status != status_filter:
                    continue

            # å…³é”®è¯è¿‡æ»¤ (æœç´¢ content å’Œ memory_content)
            if keyword:
                content = record.get("content", "")
                memory_content = metadata.get("memory_content", "")
                keyword_lower = keyword.lower()

                if (keyword_lower not in content.lower() and
                    keyword_lower not in memory_content.lower()):
                    continue

            filtered.append(record)

        return filtered

    async def _get_memory_detail(self, memory_id: str) -> Optional[Dict[str, Any]]:
        """
        è·å–å•ä¸ªè®°å¿†è¯¦æƒ… - ä¿®å¤: ç»Ÿä¸€ä½¿ç”¨ Faiss æ–‡æ¡£å­˜å‚¨
        """
        # å°è¯•æŒ‰æ–‡æ¡£IDæŸ¥è¯¢
        try:
            doc_id = int(memory_id)
        except ValueError:
            # å¦‚æœä¸æ˜¯æ•´æ•°,å°è¯•æŒ‰ memory_id æŸ¥è¯¢
            doc_id = None

        try:
            if doc_id is not None:
                # æŒ‰æ•´æ•° ID æŸ¥è¯¢
                docs = await self.faiss_manager.db.document_storage.get_documents(
                    ids=[doc_id]
                )
            else:
                # æŒ‰ memory_id æŸ¥è¯¢ (åœ¨ metadata ä¸­)
                all_docs = await self.faiss_manager.get_memories_paginated(
                    page_size=10000, offset=0
                )
                docs = [
                    doc for doc in all_docs
                    if doc.get("metadata", {}).get("memory_id") == memory_id
                ]

            if not docs:
                return None

            # metadata å·²ç»æ˜¯å­—å…¸,ç›´æ¥è¿”å›
            return self._format_memory(docs[0], source="faiss")

        except Exception as exc:
            logger.error(f"æŸ¥è¯¢è®°å¿†è¯¦æƒ…å¤±è´¥: {exc}", exc_info=True)
            return None

    def _format_memory(self, raw: Dict[str, Any], source: str) -> Dict[str, Any]:
        if source == "storage":
            memory_json = raw.get("memory_data") or "{}"
            parsed = self._safe_json_loads(memory_json)
            metadata = parsed.get("metadata", {})
            access_info = metadata.get("access_info", {})

            summary = (
                parsed.get("summary")
                or parsed.get("description")
                or parsed.get("memory_content")
                or ""
            )
            created_at = parsed.get("timestamp") or raw.get("timestamp")
            last_access = access_info.get("last_accessed_timestamp")

            return {
                "doc_id": None,
                "memory_id": raw.get("memory_id"),
                "summary": summary,
                "memory_type": raw.get("memory_type"),
                "importance": raw.get("importance_score"),
                "status": raw.get("status"),
                "created_at": self._format_timestamp(created_at),
                "last_access": self._format_timestamp(last_access),
                "source": "storage",
                "metadata": metadata,
                "raw": parsed,
                "raw_json": memory_json,
            }

        # Faiss source çš„æ ¼å¼åŒ– (ä¿®å¤: metadata ç°åœ¨å·²ç»æ˜¯å­—å…¸)
        metadata = raw.get("metadata", {})  # âœ… å·²ç»æ˜¯å­—å…¸,ä¸éœ€è¦ safe_parse_metadata

        # ä¼˜å…ˆä½¿ç”¨ metadata.memory_content,fallback åˆ° content
        summary = metadata.get("memory_content") or raw.get("content") or ""
        importance = metadata.get("importance")
        event_type = metadata.get("event_type")
        status = metadata.get("status", "active")
        created_at = metadata.get("create_time")
        last_access = metadata.get("last_access_time")

        return {
            "doc_id": raw.get("id"),
            "memory_id": metadata.get("memory_id"),
            "summary": summary,
            "memory_type": event_type,
            "importance": importance,
            "status": status,
            "created_at": self._format_timestamp(created_at),
            "last_access": self._format_timestamp(last_access),
            "source": "faiss",
            "metadata": metadata,
            "raw": {
                "content": raw.get("content"),
                "metadata": metadata,
            },
            "raw_json": json.dumps(metadata, ensure_ascii=False),
        }

    async def _gather_statistics(self) -> Tuple[int, Dict[str, int]]:
        """
        ç»Ÿè®¡è®°å¿†æ•°é‡ - ä¿®å¤: ç»Ÿä¸€ä½¿ç”¨ Faiss æ–‡æ¡£å­˜å‚¨
        """
        total = await self.faiss_manager.count_total_memories()
        counts = await self._collect_status_counts()
        return total, counts

    async def _collect_status_counts(self) -> Dict[str, int]:
        """
        é’ˆå¯¹ Faiss æ–‡æ¡£å­˜å‚¨ç»Ÿè®¡ä¸åŒçŠ¶æ€çš„è®°å¿†æ•°é‡ã€‚
        """
        counts: Dict[str, int] = {"active": 0, "archived": 0, "deleted": 0}
        try:
            conn = self.faiss_manager.db.document_storage.connection
            async with conn.execute(
                "SELECT json_extract(metadata, '$.status') AS status FROM documents"
            ) as cursor:
                rows = await cursor.fetchall()
            for row in rows:
                status_value = row[0] if row and row[0] else "active"
                counts[status_value] = counts.get(status_value, 0) + 1
        except Exception as exc:
            logger.error(f"ç»Ÿè®¡è®°å¿†çŠ¶æ€å¤±è´¥: {exc}", exc_info=True)
        return counts

    def _serialize_nuke_status(
        self,
        payload: Optional[Dict[str, Any]],
        now: Optional[float] = None,
        already_pending: bool = False,
    ) -> Dict[str, Any]:
        if not payload:
            return {"pending": False}

        now = now or time.time()
        execute_at = float(payload.get("execute_at", now))
        seconds_left = max(0, int(round(execute_at - now)))
        if already_pending:
            detail = "A pending wipe is already counting down"
        else:
            detail = (
                f"Wipe executes in {seconds_left} seconds"
                if seconds_left
                else "Wipe executing now"
            )

        return {
            "pending": True,
            "operation_id": payload.get("id"),
            "execute_at": datetime.fromtimestamp(execute_at).isoformat(
                sep=" ", timespec="seconds"
            ),
            "seconds_left": seconds_left,
            "detail": detail,
            "already_pending": already_pending,
        }

    async def _schedule_nuke(self, delay_seconds: int) -> Dict[str, Any]:
        delay = max(5, min(int(delay_seconds), 600))
        task_to_cancel: Optional[asyncio.Task] = None
        pending_snapshot: Dict[str, Any]

        async with self._nuke_lock:
            now = time.time()
            if self._pending_nuke and self._pending_nuke.get("status") == "scheduled":
                return self._serialize_nuke_status(self._pending_nuke, now, True)

            if self._nuke_task and not self._nuke_task.done():
                task_to_cancel = self._nuke_task

            operation_id = secrets.token_urlsafe(8)
            execute_at = now + delay
            pending = {
                "id": operation_id,
                "created_at": now,
                "execute_at": execute_at,
                "status": "scheduled",
            }
            self._pending_nuke = pending
            self._nuke_task = asyncio.create_task(self._run_nuke(operation_id, delay))
            pending_snapshot = dict(pending)

        if task_to_cancel:
            task_to_cancel.cancel()
            try:
                await task_to_cancel
            except asyncio.CancelledError:
                pass

        return self._serialize_nuke_status(pending_snapshot, time.time())

    async def _get_pending_nuke(self) -> Dict[str, Any]:
        async with self._nuke_lock:
            pending = self._pending_nuke
            if not pending or pending.get("status") != "scheduled":
                return {"pending": False}
            snapshot = dict(pending)
        return self._serialize_nuke_status(snapshot)

    async def _cancel_nuke(self, operation_id: str) -> bool:
        task: Optional[asyncio.Task] = None
        async with self._nuke_lock:
            if not self._pending_nuke or self._pending_nuke.get("id") != operation_id:
                return False
            task = self._nuke_task
            self._pending_nuke = None
            self._nuke_task = None

        if task and not task.done():
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass
        return True

    async def _run_nuke(self, operation_id: str, delay: int):
        try:
            await asyncio.sleep(delay)
            await self._execute_nuke(operation_id)
        except asyncio.CancelledError:
            raise
        except Exception as exc:  # pragma: no cover
            logger.error("Nuke job failed: %s", exc, exc_info=True)
            async with self._nuke_lock:
                if self._pending_nuke and self._pending_nuke.get("id") == operation_id:
                    self._pending_nuke = None
                self._nuke_task = None

    async def _execute_nuke(self, operation_id: str):
        async with self._nuke_lock:
            if not self._pending_nuke or self._pending_nuke.get("id") != operation_id:
                return
            self._pending_nuke["status"] = "running"

        vector_deleted = 0
        storage_deleted = 0

        # ğŸ­ æ ¸çˆ†åŠŸèƒ½ä»…ä¸ºè§†è§‰æ•ˆæœï¼Œä¸ä¼šçœŸå®åˆ é™¤æ•°æ®
        # è¿™æ˜¯ä¸€ä¸ªå¨±ä¹æ€§çš„è§†è§‰ç‰¹æ•ˆï¼Œä¿æŠ¤ç”¨æˆ·æ•°æ®å®‰å…¨
        try:
            # åªç»Ÿè®¡æ•°é‡ç”¨äºæ˜¾ç¤ºï¼Œä¸æ‰§è¡Œä»»ä½•åˆ é™¤æ“ä½œ
            logger.info("æ ¸çˆ†è§†è§‰æ•ˆæœè§¦å‘ï¼šè¿™åªæ˜¯æ¨¡æ‹Ÿï¼Œä¸ä¼šåˆ é™¤ä»»ä½•æ•°æ®")

            # ç»Ÿè®¡è®°å¿†æ•°é‡ç”¨äºæ˜¾ç¤º
            async with self.faiss_manager.db.document_storage.connection.execute(
                "SELECT COUNT(*) FROM documents"
            ) as cursor:
                row = await cursor.fetchone()
                vector_deleted = row[0] if row else 0

            if self.memory_storage:
                async with self.memory_storage.connection.execute(
                    "SELECT COUNT(*) FROM memories"
                ) as cursor:
                    row = await cursor.fetchone()
                    storage_deleted = row[0] if row else 0

            logger.info(
                "æ ¸çˆ†è§†è§‰æ•ˆæœå®Œæˆï¼šæ¨¡æ‹Ÿæ¸…é™¤ %s æ¡å‘é‡è®°å½•å’Œ %s æ¡ç»“æ„åŒ–è®°å½•ï¼ˆå®é™…æ•°æ®å®Œå…¨æœªå—å½±å“ï¼‰",
                vector_deleted,
                storage_deleted,
            )
        except Exception as exc:
            logger.error("Memory wipe failed: %s", exc, exc_info=True)
        finally:
            async with self._nuke_lock:
                if self._pending_nuke and self._pending_nuke.get("id") == operation_id:
                    self._pending_nuke = None
                self._nuke_task = None

    def _auth_dependency(self):
        async def dependency(request: Request) -> str:
            token = self._extract_token(request)
            if not token:
                raise HTTPException(status.HTTP_401_UNAUTHORIZED, detail="æœªæˆæƒ")
            await self._validate_token(token)
            return token

        return dependency

    async def _validate_token(self, token: str):
        """
        éªŒè¯ token - ä¿®å¤: æ£€æŸ¥ç»å¯¹è¿‡æœŸæ—¶é—´å’Œä¼šè¯è¶…æ—¶
        """
        async with self._token_lock:
            await self._cleanup_tokens_locked()
            token_data = self._tokens.get(token)

            if not token_data:
                raise HTTPException(status.HTTP_401_UNAUTHORIZED, detail="ä¼šè¯å·²å¤±æ•ˆ")

            now = time.time()

            # æ£€æŸ¥ç»å¯¹è¿‡æœŸæ—¶é—´ (24å°æ—¶)
            if now - token_data["created_at"] > token_data["max_lifetime"]:
                self._tokens.pop(token, None)
                raise HTTPException(status.HTTP_401_UNAUTHORIZED, detail="ä¼šè¯å·²è¾¾æœ€å¤§æ—¶é•¿")

            # æ£€æŸ¥ä¼šè¯è¶…æ—¶ (æœ€åæ´»åŠ¨æ—¶é—´)
            if now - token_data["last_active"] > self.session_timeout:
                self._tokens.pop(token, None)
                raise HTTPException(status.HTTP_401_UNAUTHORIZED, detail="ä¼šè¯å·²è¿‡æœŸ")

            # æ›´æ–°æœ€åæ´»åŠ¨æ—¶é—´
            token_data["last_active"] = now

    async def _cleanup_tokens_locked(self):
        """
        æ¸…ç†è¿‡æœŸ token - ä¿®å¤: é€‚é…æ–°çš„ token æ•°æ®ç»“æ„
        """
        now = time.time()
        expired = []

        for token, token_data in self._tokens.items():
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡ç»å¯¹è¿‡æœŸæ—¶é—´æˆ–ä¼šè¯è¶…æ—¶
            if (now - token_data["created_at"] > token_data["max_lifetime"] or
                now - token_data["last_active"] > self.session_timeout):
                expired.append(token)

        for token in expired:
            self._tokens.pop(token, None)

    def _extract_token(self, request: Request) -> str:
        auth_header = request.headers.get("Authorization", "")
        if auth_header.startswith("Bearer "):
            return auth_header[7:].strip()
        cookie_token = request.cookies.get("auth_token")
        if cookie_token:
            return cookie_token.strip()
        custom_header = request.headers.get("X-Auth-Token", "")
        return custom_header.strip()

    @staticmethod
    def _safe_json_loads(payload: str) -> Dict[str, Any]:
        if not payload:
            return {}
        try:
            return json.loads(payload)
        except json.JSONDecodeError:
            return {}

    @staticmethod
    def _format_timestamp(value: Any) -> Optional[str]:
        if not value:
            return None
        if isinstance(value, (int, float)):
            return datetime.fromtimestamp(value).isoformat(sep=" ", timespec="seconds")
        if isinstance(value, str):
            try:
                return datetime.fromisoformat(value.replace("Z", "+00:00")).isoformat(
                    sep=" ", timespec="seconds"
                )
            except ValueError:
                return value
        return str(value)

    # ========== è®°å¿†ç¼–è¾‘ API å®ç° ==========
    async def _update_memory_field(self, memory_id: str, payload: Dict[str, Any]) -> Dict[str, Any]:
        """æ›´æ–°è®°å¿†å­—æ®µ"""
        field = payload.get("field")
        value = payload.get("value")
        reason = payload.get("reason", "WebUI æ‰‹åŠ¨æ›´æ–°")

        if not field or value is None:
            raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="ç¼ºå°‘å¿…è¦å‚æ•°: field å’Œ value")

        # å…è®¸ç¼–è¾‘çš„å­—æ®µ
        allowed_fields = ["content", "importance", "type", "status"]
        if field not in allowed_fields:
            raise HTTPException(status.HTTP_400_BAD_REQUEST, detail=f"ä¸æ”¯æŒç¼–è¾‘å­—æ®µ: {field}")

        try:
            # è·å–è®°å¿†è¯¦æƒ…
            detail = await self._get_memory_detail(memory_id)
            if not detail:
                raise HTTPException(status.HTTP_404_NOT_FOUND, detail="æœªæ‰¾åˆ°è®°å¿†è®°å½•")

            doc_id = detail.get("doc_id")
            if doc_id is None:
                raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="æ— æ³•è·å–è®°å¿†çš„æ–‡æ¡£ID")

            # è·å–å®Œæ•´æ–‡æ¡£
            docs = await self.faiss_manager.db.document_storage.get_documents(ids=[doc_id])
            if not docs:
                raise HTTPException(status.HTTP_404_NOT_FOUND, detail="æœªæ‰¾åˆ°æ–‡æ¡£")

            doc = docs[0]
            metadata = doc.get("metadata", {})

            # æ›´æ–°å­—æ®µ
            if field == "content":
                metadata["memory_content"] = value
            elif field == "importance":
                try:
                    metadata["importance"] = float(value)
                except (ValueError, TypeError):
                    raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="é‡è¦æ€§å¿…é¡»æ˜¯æ•°å­—")
            elif field == "type":
                metadata["event_type"] = value
            elif field == "status":
                if value not in ["active", "archived", "deleted"]:
                    raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="çŠ¶æ€å€¼æ— æ•ˆ")
                metadata["status"] = value

            # æ·»åŠ æ›´æ–°å†å²
            if "update_history" not in metadata:
                metadata["update_history"] = []

            metadata["update_history"].append({
                "timestamp": time.time(),
                "field": field,
                "old_value": detail["metadata"].get(field),
                "new_value": value,
                "reason": reason
            })

            # æ›´æ–°æ–‡æ¡£
            await self.faiss_manager.db.document_storage.update_document(
                doc_id=doc_id,
                text=doc.get("text", ""),
                metadata=metadata
            )

            return {
                "success": True,
                "message": f"æˆåŠŸæ›´æ–°å­—æ®µ: {field}",
                "updated_field": field,
                "new_value": value
            }

        except HTTPException:
            raise
        except Exception as exc:
            logger.error(f"æ›´æ–°è®°å¿†å­—æ®µå¤±è´¥: {exc}", exc_info=True)
            raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, detail="æ›´æ–°å¤±è´¥") from exc

    # ========== ç³»ç»Ÿç®¡ç† API å®ç° ==========
    async def _trigger_forgetting_agent(self) -> Dict[str, Any]:
        """æ‰‹åŠ¨è§¦å‘é—å¿˜ä»£ç†"""
        if not self.forgetting_agent:
            raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="é—å¿˜ä»£ç†æœªåˆå§‹åŒ–")

        try:
            # è§¦å‘é—å¿˜ä»»åŠ¡ï¼Œç›´æ¥è¿”å›åŸå§‹ç»“æœ
            result = await self.forgetting_agent.trigger_manual_run()
            return result
        except Exception as exc:
            logger.error(f"è§¦å‘é—å¿˜ä»£ç†å¤±è´¥: {exc}", exc_info=True)
            raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, detail="é—å¿˜ä»£ç†æ‰§è¡Œå¤±è´¥") from exc

    async def _rebuild_sparse_index(self) -> Dict[str, Any]:
        """é‡å»ºç¨€ç–ç´¢å¼•"""
        if not self.sparse_retriever:
            raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="ç¨€ç–æ£€ç´¢å™¨æœªå¯ç”¨")

        try:
            # è·å–æ‰€æœ‰è®°å¿†æ•°é‡
            total = await self.faiss_manager.count_total_memories()
            
            # ç›´æ¥è°ƒç”¨ rebuild_indexï¼Œå®ƒä¼šè‡ªåŠ¨æ¸…ç©ºå¹¶é‡å»ºç´¢å¼•
            await self.sparse_retriever.rebuild_index()

            return {
                "success": True,
                "message": "ç¨€ç–ç´¢å¼•é‡å»ºå®Œæˆ",
                "indexed_count": total
            }
        except Exception as exc:
            logger.error(f"é‡å»ºç¨€ç–ç´¢å¼•å¤±è´¥: {exc}", exc_info=True)
            raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, detail="é‡å»ºç´¢å¼•å¤±è´¥") from exc

    async def _get_sessions_info(self) -> Dict[str, Any]:
        """è·å–ä¼šè¯ä¿¡æ¯"""
        if not self.session_manager:
            return {
                "total_sessions": 0,
                "sessions": [],
                "max_sessions": 0,
                "session_ttl": 0
            }

        try:
            sessions = []
            for session_id, session_data in self.session_manager._sessions.items():
                last_access = self.session_manager._access_times.get(session_id, 0)
                sessions.append({
                    "session_id": session_id,
                    "round_count": session_data.get("round_count", 0),
                    "history_size": len(session_data.get("history", [])),
                    "last_access": self._format_timestamp(last_access)
                })

            # æŒ‰æœ€åè®¿é—®æ—¶é—´æ’åº
            sessions.sort(key=lambda x: x.get("last_access", ""), reverse=True)

            return {
                "total_sessions": len(sessions),
                "sessions": sessions,
                "max_sessions": self.session_manager.max_sessions,
                "session_ttl": self.session_manager.session_ttl
            }
        except Exception as exc:
            logger.error(f"è·å–ä¼šè¯ä¿¡æ¯å¤±è´¥: {exc}", exc_info=True)
            raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, detail="è·å–ä¼šè¯ä¿¡æ¯å¤±è´¥") from exc

    # ========== è°ƒè¯•å·¥å…· API å®ç° ==========
    async def _test_search(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """æµ‹è¯•æ£€ç´¢åŠŸèƒ½"""
        if not self.recall_engine:
            raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="æ£€ç´¢å¼•æ“æœªåˆå§‹åŒ–")

        query = payload.get("query", "").strip()
        if not query:
            raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="æŸ¥è¯¢ä¸èƒ½ä¸ºç©º")

        mode = payload.get("mode", "hybrid")
        top_k = payload.get("top_k", 5)

        if mode not in ["dense", "sparse", "hybrid"]:
            raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="æ— æ•ˆçš„æ£€ç´¢æ¨¡å¼")

        try:
            # ä¿å­˜åŸå§‹é…ç½®
            original_mode = self.recall_engine.config.get("mode")
            original_top_k = self.recall_engine.config.get("top_k")

            # ä¸´æ—¶æ›´æ”¹é…ç½®
            self.recall_engine.config["mode"] = mode
            self.recall_engine.config["top_k"] = top_k

            start_time = time.time()
            # ç›´æ¥ä½¿ç”¨ faiss_manager çš„ contextï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¼  None
            context = getattr(self.faiss_manager, 'context', None)
            results = await self.recall_engine.recall(context, query)
            elapsed_time = time.time() - start_time

            # æ¢å¤é…ç½®
            self.recall_engine.config["mode"] = original_mode
            self.recall_engine.config["top_k"] = original_top_k

            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for mem in results:
                formatted_results.append({
                    "content": mem.get("metadata", {}).get("memory_content") or mem.get("content", ""),
                    "importance": mem.get("metadata", {}).get("importance"),
                    "score": mem.get("score", 0),
                    "memory_type": mem.get("metadata", {}).get("event_type"),
                    "created_at": self._format_timestamp(mem.get("metadata", {}).get("create_time"))
                })

            return {
                "query": query,
                "mode": mode,
                "top_k": top_k,
                "elapsed_time": round(elapsed_time, 3),
                "result_count": len(formatted_results),
                "results": formatted_results
            }

        except Exception as exc:
            logger.error(f"æµ‹è¯•æ£€ç´¢å¤±è´¥: {exc}", exc_info=True)
            raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, detail="æ£€ç´¢æµ‹è¯•å¤±è´¥") from exc

    async def _compare_fusion_strategies(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ¯”èåˆç­–ç•¥"""
        if not self.recall_engine:
            raise HTTPException(status.HTTP_503_SERVICE_UNAVAILABLE, detail="æ£€ç´¢å¼•æ“æœªåˆå§‹åŒ–")

        query = payload.get("query", "").strip()
        if not query:
            raise HTTPException(status.HTTP_400_BAD_REQUEST, detail="æŸ¥è¯¢ä¸èƒ½ä¸ºç©º")

        strategies = payload.get("strategies", ["rrf", "hybrid_rrf", "weighted"])
        top_k = payload.get("top_k", 5)

        try:
            # ä¿å­˜åŸå§‹é…ç½®
            original_strategy = self.recall_engine.config.get("fusion", {}).get("strategy")
            original_top_k = self.recall_engine.config.get("top_k")

            comparison_results = []
            context = getattr(self.faiss_manager, 'context', None)

            for strategy in strategies:
                # è®¾ç½®ç­–ç•¥
                self.recall_engine.config.setdefault("fusion", {})["strategy"] = strategy
                self.recall_engine.config["top_k"] = top_k
                self.recall_engine.config["mode"] = "hybrid"

                start_time = time.time()
                results = await self.recall_engine.recall(context, query)
                elapsed_time = time.time() - start_time

                comparison_results.append({
                    "strategy": strategy,
                    "elapsed_time": round(elapsed_time, 3),
                    "result_count": len(results),
                    "top_3_scores": [r.get("score", 0) for r in results[:3]]
                })

            # æ¢å¤é…ç½®
            if original_strategy:
                self.recall_engine.config["fusion"]["strategy"] = original_strategy
            self.recall_engine.config["top_k"] = original_top_k

            return {
                "query": query,
                "strategies_tested": len(strategies),
                "comparison": comparison_results
            }

        except Exception as exc:
            logger.error(f"å¯¹æ¯”èåˆç­–ç•¥å¤±è´¥: {exc}", exc_info=True)
            raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, detail="ç­–ç•¥å¯¹æ¯”å¤±è´¥") from exc

    async def _analyze_memories(self) -> Dict[str, Any]:
        """åˆ†æè®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # è·å–æ‰€æœ‰è®°å¿†
            total = await self.faiss_manager.count_total_memories()
            memories = await self.faiss_manager.get_memories_paginated(page_size=total, offset=0)

            # ç»Ÿè®¡åˆ†æ
            importance_distribution = {"0-3": 0, "3-5": 0, "5-7": 0, "7-10": 0}
            type_distribution = {}
            status_distribution = {"active": 0, "archived": 0, "deleted": 0}

            total_importance = 0
            count_with_importance = 0

            for mem in memories:
                metadata = mem.get("metadata", {})

                # é‡è¦æ€§åˆ†å¸ƒ
                importance = metadata.get("importance", 0)
                if importance is not None:
                    total_importance += importance
                    count_with_importance += 1

                    if importance < 3:
                        importance_distribution["0-3"] += 1
                    elif importance < 5:
                        importance_distribution["3-5"] += 1
                    elif importance < 7:
                        importance_distribution["5-7"] += 1
                    else:
                        importance_distribution["7-10"] += 1

                # ç±»å‹åˆ†å¸ƒ
                mem_type = metadata.get("event_type", "unknown")
                type_distribution[mem_type] = type_distribution.get(mem_type, 0) + 1

                # çŠ¶æ€åˆ†å¸ƒ
                status = metadata.get("status", "active")
                status_distribution[status] = status_distribution.get(status, 0) + 1

            avg_importance = total_importance / count_with_importance if count_with_importance > 0 else 0

            return {
                "total_memories": total,
                "average_importance": round(avg_importance, 2),
                "importance_distribution": importance_distribution,
                "type_distribution": type_distribution,
                "status_distribution": status_distribution
            }

        except Exception as exc:
            logger.error(f"åˆ†æè®°å¿†å¤±è´¥: {exc}", exc_info=True)
            raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, detail="è®°å¿†åˆ†æå¤±è´¥") from exc
